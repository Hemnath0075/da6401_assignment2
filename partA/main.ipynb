{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eb6587d",
   "metadata": {},
   "source": [
    "Question 1 (5 Marks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb44f82",
   "metadata": {},
   "source": [
    "Build a small CNN model consisting of 5 convolution layers. Each convolution layer would be followed by an activation and a max-pooling layer.\n",
    "\n",
    "After 5 such conv-activation-maxpool blocks, you should have one dense layer followed by the output layer containing 10 neurons (1 for each of the 10 classes). The input layer should be compatible with the images in the iNaturalist dataset dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f258c1",
   "metadata": {},
   "source": [
    "## Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef26318",
   "metadata": {},
   "source": [
    "We are going to first split the inaturalist_12K dataset so we are going to split the training dataset into 80% as training and 20% and validation \n",
    "\n",
    "- Each Folder Contains in training consists of 10 Folder with 1000 images each \n",
    "- Training - 800 Images and Validation 200 Images\n",
    "- I am saving training, validation and test in seperate folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1682de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 class folders: ['.DS_Store', 'Aves', 'Insecta', 'Animalia', 'Mammalia', 'Plantae', 'Fungi', 'Amphibia', 'Arachnida', 'Mollusca', 'Reptilia']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing classes:   0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing classes:  18%|█▊        | 2/11 [00:01<00:06,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing classes:  27%|██▋       | 3/11 [00:03<00:08,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing classes:  36%|███▋      | 4/11 [00:05<00:10,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing classes:  45%|████▌     | 5/11 [00:07<00:09,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing classes:  55%|█████▍    | 6/11 [00:09<00:09,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "799\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing classes:  64%|██████▎   | 7/11 [00:11<00:07,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing classes:  73%|███████▎  | 8/11 [00:13<00:05,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing classes:  82%|████████▏ | 9/11 [00:15<00:03,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing classes:  91%|█████████ | 10/11 [00:17<00:01,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing classes: 100%|██████████| 11/11 [00:19<00:00,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is splitted into train of 80% and validation of 20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def split_dataset(\n",
    "    src_dir,\n",
    "    dest_train_dir='train',\n",
    "    dest_val_dir='val',\n",
    "    train_ratio=0.8,\n",
    "    seed=42\n",
    "):\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Create destination directories if they don't exist\n",
    "    os.makedirs(dest_train_dir, exist_ok=True)\n",
    "    os.makedirs(dest_val_dir, exist_ok=True)\n",
    "\n",
    "    classes = os.listdir(src_dir)\n",
    "    print(f\"Found {len(classes)} class folders: {classes}\")\n",
    "\n",
    "    for cls in tqdm(classes, desc=\"Processing classes\"):\n",
    "        src_cls_path = os.path.join(src_dir, cls)\n",
    "        if not os.path.isdir(src_cls_path):\n",
    "            continue\n",
    "\n",
    "        all_images = [img for img in os.listdir(src_cls_path) if img.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        random.shuffle(all_images)\n",
    "\n",
    "        total_images = len(all_images)\n",
    "        train_count = int(train_ratio * total_images)\n",
    "        val_count = total_images - train_count\n",
    "        print(train_count)\n",
    "        print(val_count)\n",
    "\n",
    "        train_images = all_images[:train_count]\n",
    "        val_images = all_images[train_count:]\n",
    "\n",
    "        train_cls_path = os.path.join(dest_train_dir, cls)\n",
    "        val_cls_path = os.path.join(dest_val_dir, cls)\n",
    "\n",
    "        os.makedirs(train_cls_path, exist_ok=True)\n",
    "        os.makedirs(val_cls_path, exist_ok=True)\n",
    "\n",
    "        for img in train_images:\n",
    "            shutil.copy(os.path.join(src_cls_path, img), os.path.join(train_cls_path, img))\n",
    "\n",
    "        for img in val_images:\n",
    "            shutil.copy(os.path.join(src_cls_path, img), os.path.join(val_cls_path, img))\n",
    "\n",
    "    print(\"Dataset is splitted into train of 80% and validation of 20%\")\n",
    "\n",
    "# Usage\n",
    "split_dataset(src_dir=\"inaturalist_12K/train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2528f2",
   "metadata": {},
   "source": [
    "Question 1 (5 Marks)\n",
    "Build a small CNN model consisting of 5 convolution layers. Each convolution layer would be followed by an activation and a max-pooling layer.\n",
    "\n",
    "After 5 such conv-activation-maxpool blocks, you should have one dense layer followed by the output layer containing 10 neurons (1 for each of the 10 classes). The input layer should be compatible with the images in the iNaturalist dataset dataset.\n",
    "\n",
    "The code should be flexible such that the number of filters, size of filters, and activation function of the convolution layers and dense layers can be changed. You should also be able to change the number of neurons in the dense layer.\n",
    "\n",
    "What is the total number of computations done by your network? (assume mmm filters in each layer of size k×kk\\times kk×k and nnn neurons in the dense layer)\n",
    "What is the total number of parameters in your network? (assume mmm filters in each layer of size k×kk\\times kk×k and nnn neurons in the dense layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e896dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "SmallCNN                                 [1, 10]                   --\n",
       "├─Sequential: 1-1                        [1, 256, 7, 7]            --\n",
       "│    └─Conv2d: 2-1                       [1, 32, 224, 224]         896\n",
       "│    └─ReLU: 2-2                         [1, 32, 224, 224]         --\n",
       "│    └─MaxPool2d: 2-3                    [1, 32, 112, 112]         --\n",
       "│    └─Conv2d: 2-4                       [1, 64, 112, 112]         18,496\n",
       "│    └─ReLU: 2-5                         [1, 64, 112, 112]         --\n",
       "│    └─MaxPool2d: 2-6                    [1, 64, 56, 56]           --\n",
       "│    └─Conv2d: 2-7                       [1, 128, 56, 56]          73,856\n",
       "│    └─ReLU: 2-8                         [1, 128, 56, 56]          --\n",
       "│    └─MaxPool2d: 2-9                    [1, 128, 28, 28]          --\n",
       "│    └─Conv2d: 2-10                      [1, 256, 28, 28]          295,168\n",
       "│    └─ReLU: 2-11                        [1, 256, 28, 28]          --\n",
       "│    └─MaxPool2d: 2-12                   [1, 256, 14, 14]          --\n",
       "│    └─Conv2d: 2-13                      [1, 256, 14, 14]          590,080\n",
       "│    └─ReLU: 2-14                        [1, 256, 14, 14]          --\n",
       "│    └─MaxPool2d: 2-15                   [1, 256, 7, 7]            --\n",
       "├─Linear: 1-2                            [1, 512]                  6,423,040\n",
       "├─ReLU: 1-3                              [1, 512]                  --\n",
       "├─Linear: 1-4                            [1, 10]                   5,130\n",
       "==========================================================================================\n",
       "Total params: 7,406,666\n",
       "Trainable params: 7,406,666\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 862.08\n",
       "==========================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 24.49\n",
       "Params size (MB): 29.63\n",
       "Estimated Total Size (MB): 54.72\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "\n",
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(SmallCNN, self).__init__()\n",
    "\n",
    "        in_channels = 3  # RGB\n",
    "        conv_layers = []\n",
    "\n",
    "        for i in range(5):\n",
    "            conv_layers.append(nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=config['conv_filters'][i],\n",
    "                kernel_size=config['kernel_sizes'][i],\n",
    "                padding=1\n",
    "            ))\n",
    "            conv_layers.append(config['conv_activation']())\n",
    "            conv_layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            in_channels = config['conv_filters'][i]\n",
    "\n",
    "        self.conv_block = nn.Sequential(*conv_layers)\n",
    "\n",
    "        dummy_input = torch.zeros(1, 3, *config['image_size'])\n",
    "        dummy_output = self.conv_block(dummy_input)\n",
    "        flat_size = dummy_output.view(1, -1).shape[1]\n",
    "\n",
    "        self.fc1 = nn.Linear(flat_size, config['dense_neurons'])\n",
    "        self.act_dense = config['dense_activation']()\n",
    "        self.output = nn.Linear(config['dense_neurons'], config['num_classes'])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.act_dense(self.fc1(x))\n",
    "        return self.output(x)\n",
    "    \n",
    "config = {\n",
    "    'conv_filters': [32, 64, 128, 256, 256],\n",
    "    'kernel_sizes': [3, 3, 3, 3, 3],\n",
    "    'conv_activation': nn.ReLU,\n",
    "    'dense_activation': nn.ReLU,\n",
    "    'dense_neurons': 512,\n",
    "    'num_classes': 10,\n",
    "    'image_size': (224, 224)\n",
    "}\n",
    "\n",
    "model = SmallCNN(config)\n",
    "summary(model, input_size=(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f17effa",
   "metadata": {},
   "source": [
    "Layer 1: ConvBlock1 | In: 3 | Out: 32 | Kernel: 3 | Activation: ReLU | Pooling: MaxPool2d(2x2)\n",
    "\n",
    "Layer 2: ConvBlock2 | In: 32 | Out: 64 | Kernel: 3 | Activation: ReLU | Pooling: MaxPool2d(2x2)\n",
    "\n",
    "Layer 3: ConvBlock3 | In: 64 | Out: 128 | Kernel: 3 | Activation: ReLU | Pooling: MaxPool2d(2x2)\n",
    "\n",
    "Layer 4: ConvBlock4 | In: 128 | Out: 256 | Kernel: 3 | Activation: ReLU | Pooling: MaxPool2d(2x2)\n",
    "\n",
    "Layer 5: ConvBlock5 | In: 256 | Out: 256 | Kernel: 3 | Activation: ReLU | Pooling: MaxPool2d(2x2)\n",
    "\n",
    "Dense Layer: 512 neurons | Activation: ReLU\n",
    "\n",
    "Output Layer: 10 neurons for 10 classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dc9d6d",
   "metadata": {},
   "source": [
    "## Training and Prediction\n",
    "\n",
    "Here I have wrote the code for training and prediction\n",
    "\n",
    "- Using Adam as optimizer\n",
    "- Using Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b54784e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class TrainAndPredict:\n",
    "    def __init__(self, model, device, class_names, lr=0.001):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.class_names = class_names\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "    def train(self, train_loader, val_loader, epochs=10):\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "\n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            val_acc = self.validate(val_loader)\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | Loss: {total_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    def validate(self, val_loader):\n",
    "        self.model.eval()\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(images)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        return 100 * correct / total\n",
    "\n",
    "    def predict(self, image_tensor):\n",
    "        self.model.eval()\n",
    "        image_tensor = image_tensor.to(self.device).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(image_tensor)\n",
    "            _, pred = torch.max(outputs, 1)\n",
    "        \n",
    "        return self.class_names[pred.item()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90afffaf",
   "metadata": {},
   "source": [
    "## Training with Base model\n",
    "\n",
    "- Running the model with Guess Config Parameters here \n",
    "- Got base Accuracy but this does not perform well in test dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d4ec602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Loss: 293.8244 | Val Acc: 15.48%\n",
      "Epoch 2/10 | Loss: 286.1400 | Val Acc: 20.59%\n",
      "Epoch 3/10 | Loss: 275.8431 | Val Acc: 23.78%\n",
      "Epoch 4/10 | Loss: 265.9659 | Val Acc: 27.30%\n",
      "Epoch 5/10 | Loss: 252.5008 | Val Acc: 30.25%\n",
      "Epoch 6/10 | Loss: 241.8466 | Val Acc: 30.35%\n",
      "Epoch 7/10 | Loss: 233.7809 | Val Acc: 33.44%\n",
      "Epoch 8/10 | Loss: 225.2472 | Val Acc: 35.98%\n",
      "Epoch 9/10 | Loss: 218.3937 | Val Acc: 37.24%\n",
      "Epoch 10/10 | Loss: 201.1552 | Val Acc: 37.05%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    'conv_filters': [32, 64, 128, 256, 256],\n",
    "    'kernel_sizes': [3, 3, 3, 3, 3],\n",
    "    'conv_activation': nn.ReLU,\n",
    "    'dense_activation': nn.ReLU,\n",
    "    'dense_neurons': 512,\n",
    "    'num_classes': 10,\n",
    "    'image_size': (224, 224)\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load data\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(config['image_size']),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder('train', transform=transform)\n",
    "val_dataset = datasets.ImageFolder('val', transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize\n",
    "model = SmallCNN(config)\n",
    "trainer = TrainAndPredict(model, device, train_dataset.classes)\n",
    "\n",
    "# Train\n",
    "trainer.train(train_loader, val_loader, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e581f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "activations = {\n",
    "    'relu': nn.ReLU(),\n",
    "    'tanh': nn.Tanh(),\n",
    "    'sigmoid': nn.Sigmoid(),\n",
    "    'leaky_relu': nn.LeakyReLU(),\n",
    "    'mish':nn.Mish(),\n",
    "    'gelu':nn.GELU(),\n",
    "    'silu':nn.SiLU(),\n",
    "    'relu6':nn.ReLU6()\n",
    "}\n",
    "\n",
    "optimizer_dict = {\n",
    "    'adam': optim.Adam,\n",
    "    'adamw': optim.AdamW,\n",
    "    'sgd': optim.SGD\n",
    "}\n",
    "\n",
    "\n",
    "def generate_filters(base_m, strategy):\n",
    "            if strategy == 'same':\n",
    "                return [base_m] * 5\n",
    "            elif strategy == 'double':\n",
    "                return [base_m * (2 ** i) for i in range(5)]\n",
    "            elif strategy == 'half':\n",
    "                return [max(1, base_m // (2 ** i)) for i in range(5)]\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        in_channels = config['input_dimension'][0]\n",
    "        base_m = config['conv_filters']\n",
    "        strategy = config['filter_org']\n",
    "        conv_filters = generate_filters(base_m, strategy)\n",
    "        kernel_sizes = config['kernel_sizes']\n",
    "        stride = config['stride']\n",
    "        padding = config['padding']\n",
    "        pool = config['max_pooling_size']\n",
    "        dropout = config['dropout_rate']\n",
    "        use_bn = config['use_batchnorm']\n",
    "        dropout_org = config['dropout_organisation']\n",
    "\n",
    "        conv_layers = []\n",
    "        for i in range(5):  # 5 conv layers\n",
    "            out_channels = conv_filters[i]\n",
    "            conv_layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_sizes[i], stride=stride, padding=padding))\n",
    "            if use_bn:\n",
    "                conv_layers.append(nn.BatchNorm2d(out_channels))\n",
    "            if dropout_org == 'before_relu':\n",
    "                conv_layers.append(nn.Dropout2d(dropout))\n",
    "            conv_layers.append(activations[config['conv_activation']])\n",
    "            if dropout_org == 'after_relu':\n",
    "                conv_layers.append(nn.Dropout2d(dropout))\n",
    "            conv_layers.append(nn.MaxPool2d(kernel_size=pool))\n",
    "            in_channels = out_channels\n",
    "\n",
    "        self.conv = nn.Sequential(*conv_layers)\n",
    "\n",
    "        # Estimate flattened size\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros((1, *config['input_dimension']))\n",
    "            dummy_output = self.conv(dummy_input)\n",
    "            flattened_size = dummy_output.view(1, -1).shape[1]\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(flattened_size, config['dense_neurons']),\n",
    "            activations[config['dense_activation']],\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(config['dense_neurons'], config['num_classes'])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class TrainAndPredict:\n",
    "    def __init__(self, model, device, class_names, optimizer=None, lr=0.001, weight_decay=0.0):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.class_names = class_names\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optimizer_dict[optimizer](self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    def train(self, train_loader, val_loader, epochs=10, save_path='best_model.pth'):\n",
    "        best_val_acc = 0.0\n",
    "        best_epoch = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            correct, total = 0, 0\n",
    "\n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "            train_loss = total_loss / len(train_loader)\n",
    "            train_acc = 100 * correct / total\n",
    "            val_acc = self.validate(val_loader)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "\n",
    "            # Save best model\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_epoch = epoch + 1\n",
    "                torch.save(self.model.state_dict(), save_path)\n",
    "\n",
    "                artifact = wandb.Artifact('best-model', type='model')\n",
    "                artifact.add_file(save_path)\n",
    "                wandb.log_artifact(artifact)\n",
    "\n",
    "            # Log to Weights & Biases\n",
    "            wandb.log({\n",
    "                'epoch': epoch + 1,\n",
    "                'train_loss': train_loss,\n",
    "                'train_acc': train_acc,\n",
    "                'val_acc': val_acc\n",
    "            })\n",
    "\n",
    "        print(f\"\\nBest model saved from Epoch {best_epoch} with Val Acc: {best_val_acc:.2f}%\")\n",
    "\n",
    "    def validate(self, val_loader):\n",
    "        self.model.eval()\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(images)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        return 100 * correct / total\n",
    "\n",
    "    def predict(self, image_tensor):\n",
    "        self.model.eval()\n",
    "        image_tensor = image_tensor.to(self.device).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(image_tensor)\n",
    "            _, pred = torch.max(outputs, 1)\n",
    "        \n",
    "        return self.class_names[pred.item()]\n",
    "\n",
    "\n",
    "\n",
    "def train_sweep(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        # print(config.conv_filters)\n",
    "        wandb.run.name = f\"filter_{config.filter_size}/dn_{config.n_neurons}/opt_{config.optimizer}/aug_{config.use_augmentation}\"\n",
    "\n",
    "        # Build dynamic config from sweep values\n",
    "        dynamic_config = {\n",
    "            'input_dimension': (3, 224, 224),\n",
    "            'conv_filters': config.conv_filters,\n",
    "            'kernel_sizes': [config.filter_size] * 5,\n",
    "            'stride': config.stride,\n",
    "            'filter_org': config.filter_org,\n",
    "            'padding': config.padding,\n",
    "            'max_pooling_size': config.max_pooling_size,\n",
    "            'dropout_rate': config.dropout_rate,\n",
    "            'use_batchnorm': config.use_batchnorm,\n",
    "            'factor': config.factor,\n",
    "            'dropout_organisation': 'after_relu',\n",
    "            'dense_neurons': config.n_neurons,\n",
    "            'num_classes': config.n_classes,\n",
    "            'optimizer': config.optimizer,\n",
    "            'conv_activation': config.conv_activation,\n",
    "            'dense_activation': config.dense_activation,\n",
    "            'image_size': (224, 224),\n",
    "            \n",
    "        }\n",
    "        \n",
    "        if config['filter_org'] == 'half' and config['conv_filters'] < 32:\n",
    "            print(\"Skipping config: unsafe filter_org with too few filters\")\n",
    "            return\n",
    "        if config['stride'] > 1 and config['max_pooling_size'] > 1 and config['filter_size'] >= 7:\n",
    "            print(\"Skipping config: stride/pool too aggressive with large filter\")\n",
    "            return\n",
    "\n",
    "        # Define your model\n",
    "        model = CNN(dynamic_config)\n",
    "\n",
    "        # Dataloaders\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(dynamic_config['image_size'], scale=(0.5, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=15),\n",
    "            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.05),\n",
    "            transforms.RandomGrayscale(p=0.1),\n",
    "            transforms.GaussianBlur(kernel_size=3),\n",
    "            transforms.ToTensor(),\n",
    "        ]) if config.use_augmentation else transforms.Compose([\n",
    "            transforms.Resize(dynamic_config['image_size']),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        val_transform = transforms.Compose([\n",
    "            transforms.Resize(dynamic_config['image_size']),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "        torch.cuda.set_device(device)\n",
    "        train_dataset = datasets.ImageFolder('train', transform=train_transform)\n",
    "        val_dataset = datasets.ImageFolder('val', transform=val_transform)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True,num_workers=4, pin_memory=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=True,num_workers=4, pin_memory=True)\n",
    "\n",
    "        trainer = TrainAndPredict(model, device, train_dataset.classes,optimizer=config.optimizer,lr=config.learning_rate)\n",
    "\n",
    "        # Train and log\n",
    "        trainer.train(train_loader, val_loader, epochs=config.epochs)\n",
    "        \n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'name': 'Custom CNN',\n",
    "    'metric': {'name': \"val_accuracy\", 'goal': 'maximize'},\n",
    "    'parameters': {\n",
    "        'conv_filters': {'values': [32, 64, 128]},\n",
    "        'filter_org': {\n",
    "            'values': ['same', 'double', 'half']\n",
    "        },\n",
    "        'filter_size': {'values': [1,3,7,11]},\n",
    "        'stride': {'values': [1,2]},\n",
    "        'padding': {'values': [1,2]},\n",
    "        'max_pooling_size': {'value': 2},\n",
    "        'n_neurons': {'values': [64, 128, 256, 512, 1024]},\n",
    "        'n_classes': {'value': 10},\n",
    "        'conv_activation': {\n",
    "            'values': ['relu', 'gelu', 'silu', 'mish', 'relu6','leaky_relu']\n",
    "        },\n",
    "        'dense_activation': {\n",
    "            'values': ['relu', 'gelu', 'silu', 'mish', 'relu6','leaky_relu']\n",
    "        },\n",
    "        'dropout_rate': {'values': [0.2, 0.3, 0.4, 0.5]},\n",
    "        'use_batchnorm': {'values': [True, False]},\n",
    "        'factor': {'values': [0.5, 1, 2, 3]},\n",
    "        'learning_rate': {'values': [0.001,0.0001]},\n",
    "        'batch_size': {'values': [16,32,64]},\n",
    "        'optimizer': {'values': ['adam', 'adamw','sgd']},\n",
    "        'epochs': {'values': [5,10,15]},\n",
    "        'use_augmentation': {'values': [True, False]},\n",
    "        'dropout_organisation': {'values': ['after_relu','before_relu']},  # simplified for now\n",
    "    },\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"iNaturalist_CNN\")\n",
    "wandb.agent(sweep_id, function=train_sweep, count=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d198d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "activations = {\n",
    "    'relu': nn.ReLU(),\n",
    "    'tanh': nn.Tanh(),\n",
    "    'sigmoid': nn.Sigmoid(),\n",
    "    'leaky_relu': nn.LeakyReLU(),\n",
    "    'mish':nn.Mish(),\n",
    "    'gelu':nn.GELU(),\n",
    "    'silu':nn.SiLU(),\n",
    "    'relu6':nn.ReLU6()\n",
    "}\n",
    "\n",
    "optimizer_dict = {\n",
    "    'adam': optim.Adam,\n",
    "    'adamw': optim.AdamW,\n",
    "    'sgd': optim.SGD\n",
    "}\n",
    "\n",
    "\n",
    "def generate_filters(base_m, strategy):\n",
    "            if strategy == 'same':\n",
    "                return [base_m] * 5\n",
    "            elif strategy == 'double':\n",
    "                return [base_m * (2 ** i) for i in range(5)]\n",
    "            elif strategy == 'half':\n",
    "                return [max(1, base_m // (2 ** i)) for i in range(5)]\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        in_channels = config['input_dimension'][0]\n",
    "        base_m = config['conv_filters']\n",
    "        strategy = config['filter_org']\n",
    "        conv_filters = generate_filters(base_m, strategy)\n",
    "        kernel_sizes = config['kernel_sizes']\n",
    "        stride = config['stride']\n",
    "        padding = config['padding']\n",
    "        pool = config['max_pooling_size']\n",
    "        dropout = config['dropout_rate']\n",
    "        use_bn = config['use_batchnorm']\n",
    "        dropout_org = config['dropout_organisation']\n",
    "\n",
    "        conv_layers = []\n",
    "        for i in range(5):  # 5 conv layers\n",
    "            out_channels = conv_filters[i]\n",
    "            conv_layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_sizes[i], stride=stride, padding=padding))\n",
    "            if use_bn:\n",
    "                conv_layers.append(nn.BatchNorm2d(out_channels))\n",
    "            if dropout_org == 'before_relu':\n",
    "                conv_layers.append(nn.Dropout2d(dropout))\n",
    "            conv_layers.append(activations[config['conv_activation']])\n",
    "            if dropout_org == 'after_relu':\n",
    "                conv_layers.append(nn.Dropout2d(dropout))\n",
    "            conv_layers.append(nn.MaxPool2d(kernel_size=pool))\n",
    "            in_channels = out_channels\n",
    "\n",
    "        self.conv = nn.Sequential(*conv_layers)\n",
    "\n",
    "        # Estimate flattened size\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros((1, *config['input_dimension']))\n",
    "            dummy_output = self.conv(dummy_input)\n",
    "            flattened_size = dummy_output.view(1, -1).shape[1]\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(flattened_size, config['dense_neurons']),\n",
    "            activations[config['dense_activation']],\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(config['dense_neurons'], config['num_classes'])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class TrainAndPredict:\n",
    "    def __init__(self, model, device, class_names, optimizer=None, lr=0.001, weight_decay=0.0):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.class_names = class_names\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optimizer_dict[optimizer](self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    def train(self, train_loader, val_loader, epochs=10, save_path='best_model.pth'):\n",
    "        best_val_acc = 0.0\n",
    "        best_epoch = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            correct, total = 0, 0\n",
    "\n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "            train_loss = total_loss / len(train_loader)\n",
    "            train_acc = 100 * correct / total\n",
    "            val_acc = self.validate(val_loader)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "\n",
    "            # Save best model\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_epoch = epoch + 1\n",
    "                torch.save(self.model.state_dict(), save_path)\n",
    "\n",
    "                artifact = wandb.Artifact('best-model', type='model')\n",
    "                artifact.add_file(save_path)\n",
    "                wandb.log_artifact(artifact)\n",
    "\n",
    "            # Log to Weights & Biases\n",
    "            wandb.log({\n",
    "                'epoch': epoch + 1,\n",
    "                'train_loss': train_loss,\n",
    "                'train_acc': train_acc,\n",
    "                'val_acc': val_acc\n",
    "            })\n",
    "\n",
    "        print(f\"\\nBest model saved from Epoch {best_epoch} with Val Acc: {best_val_acc:.2f}%\")\n",
    "\n",
    "    def validate(self, val_loader):\n",
    "        self.model.eval()\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(images)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        return 100 * correct / total\n",
    "\n",
    "    def predict(self, image_tensor):\n",
    "        self.model.eval()\n",
    "        image_tensor = image_tensor.to(self.device).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(image_tensor)\n",
    "            _, pred = torch.max(outputs, 1)\n",
    "        \n",
    "        return self.class_names[pred.item()]\n",
    "\n",
    "\n",
    "\n",
    "def train_sweep(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        # print(config.conv_filters)\n",
    "        wandb.run.name = f\"filter_{config.filter_size}/dn_{config.n_neurons}/opt_{config.optimizer}/aug_{config.use_augmentation}\"\n",
    "\n",
    "        # Build dynamic config from sweep values\n",
    "        dynamic_config = {\n",
    "            'input_dimension': (3, 224, 224),\n",
    "            'conv_filters': config.conv_filters,\n",
    "            'kernel_sizes': [config.filter_size] * 5,\n",
    "            'stride': config.stride,\n",
    "            'filter_org': config.filter_org,\n",
    "            'padding': config.padding,\n",
    "            'max_pooling_size': config.max_pooling_size,\n",
    "            'dropout_rate': config.dropout_rate,\n",
    "            'use_batchnorm': config.use_batchnorm,\n",
    "            'factor': config.factor,\n",
    "            'dropout_organisation': 'after_relu',\n",
    "            'dense_neurons': config.n_neurons,\n",
    "            'num_classes': config.n_classes,\n",
    "            'optimizer': config.optimizer,\n",
    "            'conv_activation': config.conv_activation,\n",
    "            'dense_activation': config.dense_activation,\n",
    "            'image_size': (224, 224),\n",
    "            \n",
    "        }\n",
    "        \n",
    "        if config['filter_org'] == 'half' and config['conv_filters'] < 32:\n",
    "            print(\"Skipping config: unsafe filter_org with too few filters\")\n",
    "            return\n",
    "        if config['stride'] > 1 and config['max_pooling_size'] > 1 and config['filter_size'] >= 7:\n",
    "            print(\"Skipping config: stride/pool too aggressive with large filter\")\n",
    "            return\n",
    "\n",
    "        # Define your model\n",
    "        model = CNN(dynamic_config)\n",
    "\n",
    "        # Dataloaders\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(dynamic_config['image_size'], scale=(0.5, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=15),\n",
    "            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.05),\n",
    "            transforms.RandomGrayscale(p=0.1),\n",
    "            transforms.GaussianBlur(kernel_size=3),\n",
    "            transforms.ToTensor(),\n",
    "        ]) if config.use_augmentation else transforms.Compose([\n",
    "            transforms.Resize(dynamic_config['image_size']),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        val_transform = transforms.Compose([\n",
    "            transforms.Resize(dynamic_config['image_size']),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "        torch.cuda.set_device(device)\n",
    "        train_dataset = datasets.ImageFolder('train', transform=train_transform)\n",
    "        val_dataset = datasets.ImageFolder('val', transform=val_transform)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True,num_workers=4, pin_memory=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=True,num_workers=4, pin_memory=True)\n",
    "\n",
    "        trainer = TrainAndPredict(model, device, train_dataset.classes,optimizer=config.optimizer,lr=config.learning_rate)\n",
    "\n",
    "        # Train and log\n",
    "        trainer.train(train_loader, val_loader, epochs=config.epochs)\n",
    "        \n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'name': 'Custom CNN',\n",
    "    'metric': {'name': \"val_accuracy\", 'goal': 'maximize'},\n",
    "    'parameters': {\n",
    "        'conv_filters': {'values': [32, 64, 128]},\n",
    "        'filter_org': {\n",
    "            'values': ['same', 'double', 'half']\n",
    "        },\n",
    "        'filter_size': {'values': [1,3,7,11]},\n",
    "        'stride': {'values': [1,2]},\n",
    "        'padding': {'values': [1,2]},\n",
    "        'max_pooling_size': {'value': 2},\n",
    "        'n_neurons': {'values': [64, 128, 256, 512, 1024]},\n",
    "        'n_classes': {'value': 10},\n",
    "        'conv_activation': {\n",
    "            'values': ['relu', 'gelu', 'silu', 'mish', 'relu6','leaky_relu']\n",
    "        },\n",
    "        'dense_activation': {\n",
    "            'values': ['relu', 'gelu', 'silu', 'mish', 'relu6','leaky_relu']\n",
    "        },\n",
    "        'dropout_rate': {'values': [0.2, 0.3, 0.4, 0.5]},\n",
    "        'use_batchnorm': {'values': [True, False]},\n",
    "        'factor': {'values': [0.5, 1, 2, 3]},\n",
    "        'learning_rate': {'values': [0.001,0.0001]},\n",
    "        'batch_size': {'values': [16,32,64]},\n",
    "        'optimizer': {'values': ['adam', 'adamw','sgd']},\n",
    "        'epochs': {'values': [5,10,15]},\n",
    "        'use_augmentation': {'values': [True, False]},\n",
    "        'dropout_organisation': {'values': ['after_relu','before_relu']},  # simplified for now\n",
    "    },\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"iNaturalist_CNN\")\n",
    "wandb.agent(sweep_id, function=train_sweep, count=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2360ab4",
   "metadata": {},
   "source": [
    "Question 4 (5 Marks)\n",
    "\n",
    "\n",
    "You will now apply your best model on the test data (You shouldn't have used test data so far. All the above experiments should have been done using train and validation data only).\n",
    "\n",
    "Use the best model from your sweep and report the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79f00f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mch24s016\u001b[0m (\u001b[33mch24s016-iitm\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/e_disk/ch24s016/da6401_assignment2/partA/wandb/run-20250419_231349-2o60d1fv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ch24s016-iitm/da6401_assignment2-partA/runs/2o60d1fv' target=\"_blank\">splendid-morning-1</a></strong> to <a href='https://wandb.ai/ch24s016-iitm/da6401_assignment2-partA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ch24s016-iitm/da6401_assignment2-partA' target=\"_blank\">https://wandb.ai/ch24s016-iitm/da6401_assignment2-partA</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ch24s016-iitm/da6401_assignment2-partA/runs/2o60d1fv' target=\"_blank\">https://wandb.ai/ch24s016-iitm/da6401_assignment2-partA/runs/2o60d1fv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">splendid-morning-1</strong> at: <a href='https://wandb.ai/ch24s016-iitm/da6401_assignment2-partA/runs/2o60d1fv' target=\"_blank\">https://wandb.ai/ch24s016-iitm/da6401_assignment2-partA/runs/2o60d1fv</a><br> View project at: <a href='https://wandb.ai/ch24s016-iitm/da6401_assignment2-partA' target=\"_blank\">https://wandb.ai/ch24s016-iitm/da6401_assignment2-partA</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250419_231349-2o60d1fv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/e_disk/ch24s016/da6401_assignment2/partA/wandb/run-20250419_231350-hjg3wpan</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ch24s016-iitm/iNaturalist_CNN/runs/hjg3wpan' target=\"_blank\">bestsweepsofar</a></strong> to <a href='https://wandb.ai/ch24s016-iitm/iNaturalist_CNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ch24s016-iitm/iNaturalist_CNN' target=\"_blank\">https://wandb.ai/ch24s016-iitm/iNaturalist_CNN</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ch24s016-iitm/iNaturalist_CNN/runs/hjg3wpan' target=\"_blank\">https://wandb.ai/ch24s016-iitm/iNaturalist_CNN/runs/hjg3wpan</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Run ch24s016-iitm/iNaturalist_CNN/uvxjn5sl (finished)>\n",
      "Found artifact: best-model:v423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact best-model:v423, 202.73MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifact downloaded to: downloaded_artifacts\n",
      "Best run ID: uvxjn5sl, Val Accuracy: 38.88367729831145\n",
      "the best model parameters are {'epochs': 15, 'factor': 2, 'stride': 1, 'padding': 2, 'n_classes': 10, 'n_neurons': 512, 'optimizer': 'adam', 'batch_size': 64, 'filter_org': 'double', 'filter_size': 1, 'conv_filters': 64, 'dropout_rate': 0.5, 'learning_rate': 0.0001, 'use_batchnorm': True, 'conv_activation': 'silu', 'dense_activation': 'relu6', 'max_pooling_size': 2, 'use_augmentation': False, 'dropout_organisation': 'after_relu'}\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "wandb.init()\n",
    "wandb.init(project=\"iNaturalist_CNN\",entity=\"ch24s016-iitm\",name='bestsweepsofar')\n",
    "api = wandb.Api()\n",
    "\n",
    "# Fetch all runs in the sweep\n",
    "sweep_runs = api.sweep(f\"ch24s016-iitm/iNaturalist_CNN/xc60uj91\").runs\n",
    "\n",
    "\n",
    "# Find the best model based on validation accuracy\n",
    "best_run = max(sweep_runs, key=lambda run: run.summary.get(\"val_acc\", 0))\n",
    "\n",
    "print(best_run)\n",
    "\n",
    "artifacts = best_run.logged_artifacts()\n",
    "artifact_found = False\n",
    "\n",
    "for artifact in artifacts:\n",
    "    if artifact.name.startswith(\"best-model\"):\n",
    "        artifact_found = True\n",
    "        print(f\"Found artifact: {artifact.name}\")\n",
    "        artifact_dir = artifact.download(root=\"downloaded_artifacts\")\n",
    "        print(f\"Artifact downloaded to: {artifact_dir}\")\n",
    "        break\n",
    "\n",
    "print(f\"Best run ID: {best_run.id}, Val Accuracy: {best_run.summary.get('val_acc', 0)}\")\n",
    "\n",
    "print('the best model parameters are',best_run.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b91a9b",
   "metadata": {},
   "source": [
    "the best accuracy is Val Accuracy: 38.88367729831145\n",
    "the best model parameters are {'epochs': 15, 'factor': 2, 'stride': 1, 'padding': 2, 'n_classes': 10, 'n_neurons': 512, 'optimizer': 'adam', 'batch_size': 64, 'filter_org': 'double', 'filter_size': 1, 'conv_filters': 64, 'dropout_rate': 0.5, 'learning_rate': 0.0001, 'use_batchnorm': True, 'conv_activation': 'silu', 'dense_activation': 'relu6', 'max_pooling_size': 2, 'use_augmentation': False, 'dropout_organisation': 'after_relu'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645c8967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Downloaded 'best-model' to: ./best_model_download\n"
     ]
    }
   ],
   "source": [
    "wandb.login()\n",
    "api = wandb.Api()\n",
    "\n",
    "\n",
    "run_id = \"xc60uj91\"\n",
    "project = \"iNaturalist_CNN\"\n",
    "entity = \"ch24s016-iitm\"\n",
    "\n",
    "\n",
    "run = api.run(f\"{entity}/{project}/{run_id}\")\n",
    "\n",
    "for artifact in run.logged_artifacts():\n",
    "    if \"best-model\" in artifact.name:\n",
    "        artifact_dir = artifact.download(root=\"./best_model_download\")\n",
    "        print(f\"Downloaded 'best-model' to: {artifact_dir}\")\n",
    "        break\n",
    "else:\n",
    "    print(\"'best-model' artifact not found in this run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554d65b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 351\u001b[0m\n\u001b[1;32m    348\u001b[0m     show_predictions_grid(model, test_loader, device, test_dataset\u001b[38;5;241m.\u001b[39mclasses, um_images\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 351\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 346\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    342\u001b[0m model \u001b[38;5;241m=\u001b[39m CNN(config)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    343\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_model_cnn_final.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[0;32m--> 346\u001b[0m \u001b[43mevaluate_on_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m show_predictions_grid(model, test_loader, device, test_dataset\u001b[38;5;241m.\u001b[39mclasses, um_images\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[0;32mIn[31], line 211\u001b[0m, in \u001b[0;36mevaluate_on_test\u001b[0;34m(model, test_loader, device, class_names)\u001b[0m\n\u001b[1;32m    208\u001b[0m all_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[1;32m    212\u001b[0m         images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    213\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(images)\n",
      "File \u001b[0;32m/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/torchvision/datasets/folder.py:245\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    244\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[0;32m--> 245\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    247\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n",
      "File \u001b[0;32m/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/torchvision/datasets/folder.py:284\u001b[0m, in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/torchvision/datasets/folder.py:264\u001b[0m, in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    263\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/PIL/Image.py:982\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;15\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;24\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    980\u001b[0m     deprecate(mode, \u001b[38;5;241m12\u001b[39m)\n\u001b[0;32m--> 982\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    984\u001b[0m has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    986\u001b[0m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/PIL/ImageFile.py:341\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtile\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m_tilesort)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;66;03m# FIXME: This is a hack to handle TIFF's JpegTables tag.\u001b[39;00m\n\u001b[0;32m--> 341\u001b[0m prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtile_prefix\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;66;03m# Remove consecutive duplicates that only differ by their offset\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtile \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28mlist\u001b[39m(tiles)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, tiles \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mgroupby(\n\u001b[1;32m    347\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtile, \u001b[38;5;28;01mlambda\u001b[39;00m tile: (tile[\u001b[38;5;241m0\u001b[39m], tile[\u001b[38;5;241m1\u001b[39m], tile[\u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m    348\u001b[0m     )\n\u001b[1;32m    349\u001b[0m ]\n",
      "File \u001b[0;32m/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/PIL/JpegImagePlugin.py:400\u001b[0m, in \u001b[0;36mJpegImageFile.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    398\u001b[0m     deprecate(name, \u001b[38;5;241m12\u001b[39m)\n\u001b[1;32m    399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name)\n\u001b[0;32m--> 400\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;43;01mAttributeError\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import wandb\n",
    "import seaborn as sns\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "activations = {\n",
    "    'relu': nn.ReLU(),\n",
    "    'tanh': nn.Tanh(),\n",
    "    'sigmoid': nn.Sigmoid(),\n",
    "    'leaky_relu': nn.LeakyReLU(),\n",
    "    'mish':nn.Mish(),\n",
    "    'gelu':nn.GELU(),\n",
    "    'silu':nn.SiLU(),\n",
    "    'relu6':nn.ReLU6()\n",
    "}\n",
    "\n",
    "optimizer_dict = {\n",
    "    'adam': optim.Adam,\n",
    "    'adamw': optim.AdamW,\n",
    "    'sgd': optim.SGD\n",
    "}\n",
    "\n",
    "config = {\n",
    "    'epochs': 20,\n",
    "    'factor': 2,\n",
    "    'stride': 1,\n",
    "    'padding': 2,\n",
    "    'num_classes': 10,\n",
    "    'dense_neurons': 512,\n",
    "    'optimizer': 'adam',\n",
    "    'batch_size': 64,\n",
    "    'filter_org': 'double',\n",
    "    'filter_size': 1,\n",
    "    'conv_filters': 64,\n",
    "    'dropout_rate': 0.5,\n",
    "    'learning_rate': 0.0001,\n",
    "    'use_batchnorm': True,\n",
    "    'conv_activation': 'gelu',\n",
    "    'dense_activation': 'relu6',\n",
    "    'max_pooling_size': 2,\n",
    "    'use_augmentation': False,\n",
    "    'dropout_organisation': 'after_relu',\n",
    "    'kernel_sizes': [1, 1, 1, 1, 1],  \n",
    "    'input_dimension': (3, 224, 224),  \n",
    "    'image_size': (224, 224),\n",
    "}\n",
    "\n",
    "\n",
    "def generate_filters(base_m, strategy):\n",
    "    if strategy == 'same':\n",
    "        return [base_m] * 5\n",
    "    elif strategy == 'double':\n",
    "        return [base_m * (2 ** i) for i in range(5)]\n",
    "    elif strategy == 'half':\n",
    "        return [max(1, base_m // (2 ** i)) for i in range(5)]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "    \n",
    "class TrainAndPredict:\n",
    "    def __init__(self, model, device, class_names, optimizer=None, lr=0.001, weight_decay=0.0):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.class_names = class_names\n",
    "        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "        self.optimizer = optimizer_dict[optimizer](self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    def train(self, train_loader, val_loader, epochs=10, save_path='best_model_cnn_final.pth'):\n",
    "        best_val_acc = 0.0\n",
    "        best_epoch = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            correct, total = 0, 0\n",
    "\n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "            train_loss = total_loss / len(train_loader)\n",
    "            train_acc = 100 * correct / total\n",
    "            val_acc = self.validate(val_loader)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "\n",
    "            # Save best model bases on val accuracy but I Feel like Saving best model on high training accuracy makes it better\n",
    "            # if val_acc > best_val_acc:\n",
    "            #     best_val_acc = val_acc\n",
    "            #     best_epoch = epoch + 1\n",
    "            #     torch.save(self.model.state_dict(), save_path)\n",
    "\n",
    "            #     artifact = wandb.Artifact('best-model', type='model')\n",
    "            #     artifact.add_file(save_path)\n",
    "                # wandb.log_artifact(artifact)\n",
    "\n",
    "            # Log to Weights & Biases\n",
    "            # wandb.log({\n",
    "            #     'epoch': epoch + 1,\n",
    "            #     'train_loss': train_loss,\n",
    "            #     'train_acc': train_acc,\n",
    "            #     'val_acc': val_acc\n",
    "            # })\n",
    "            if epoch == 19:\n",
    "                torch.save(self.model.state_dict(), save_path)\n",
    "\n",
    "        print(f\"\\nBest model saved from Epoch {best_epoch} with Val Acc: {best_val_acc:.2f}%\")\n",
    "\n",
    "    def validate(self, val_loader):\n",
    "        self.model.eval()\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(images)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        return 100 * correct / total\n",
    "\n",
    "    def predict(self, image_tensor):\n",
    "        self.model.eval()\n",
    "        image_tensor = image_tensor.to(self.device).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(image_tensor)\n",
    "            _, pred = torch.max(outputs, 1)\n",
    "        \n",
    "        return self.class_names[pred.item()]\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        in_channels = config['input_dimension'][0]\n",
    "        base_m = config['conv_filters']\n",
    "        strategy = config['filter_org']\n",
    "        conv_filters = generate_filters(base_m, strategy)\n",
    "        kernel_sizes = config['kernel_sizes']\n",
    "        stride = config['stride']\n",
    "        padding = config['padding']\n",
    "        pool = config['max_pooling_size']\n",
    "        dropout = config['dropout_rate']\n",
    "        use_bn = config['use_batchnorm']\n",
    "        dropout_org = config['dropout_organisation']\n",
    "\n",
    "        conv_layers = []\n",
    "        for i in range(5):\n",
    "            out_channels = conv_filters[i]\n",
    "            conv_layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_sizes[i], stride=stride, padding=padding))\n",
    "            if use_bn:\n",
    "                conv_layers.append(nn.BatchNorm2d(out_channels))\n",
    "            # if dropout_org == 'before_relu':\n",
    "            #     conv_layers.append(nn.Dropout2d(dropout))\n",
    "            conv_layers.append(activations[config['conv_activation']])\n",
    "            # if dropout_org == 'after_relu':\n",
    "            #     conv_layers.append(nn.Dropout2d(dropout))\n",
    "            conv_layers.append(nn.MaxPool2d(kernel_size=pool))\n",
    "            in_channels = out_channels\n",
    "\n",
    "        self.conv = nn.Sequential(*conv_layers)\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros((1, *config['input_dimension']))\n",
    "            dummy_output = self.conv(dummy_input)\n",
    "            flattened_size = dummy_output.view(1, -1).shape[1]\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(flattened_size, config['dense_neurons']),\n",
    "            activations[config['dense_activation']],\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(config['dense_neurons'], config['num_classes'])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "def evaluate_on_test(model, test_loader, device, class_names=None):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    print(f\"\\nTest Accuracy: {acc:.2f}%\")\n",
    "\n",
    "    # Plot Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names if class_names else range(cm.shape[0]),\n",
    "                yticklabels=class_names if class_names else range(cm.shape[0]))\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return acc\n",
    "\n",
    "def show_predictions_grid(model, test_loader, device, class_names, num_images=10):\n",
    "    model.eval()\n",
    "    images_shown = 0\n",
    "    rows = num_images\n",
    "    fig, axs = plt.subplots(rows, 3, figsize=(10, rows * 2))\n",
    "    fig.suptitle(\"Predictions vs Actuals\", fontsize=16)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for i in range(images.size(0)):\n",
    "                if images_shown >= num_images:\n",
    "                    break\n",
    "\n",
    "                image = images[i].cpu().numpy().transpose((1, 2, 0))\n",
    "                image = np.clip(image, 0, 1)\n",
    "\n",
    "                axs[images_shown, 0].imshow(image)\n",
    "                axs[images_shown, 0].axis('off')\n",
    "                axs[images_shown, 0].set_title(\"Image\")\n",
    "\n",
    "                axs[images_shown, 1].text(0.5, 0.5, class_names[preds[i].item()], fontsize=12, ha='center')\n",
    "                axs[images_shown, 1].axis('off')\n",
    "                axs[images_shown, 1].set_title(\"Predicted\")\n",
    "\n",
    "                axs[images_shown, 2].text(0.5, 0.5, class_names[labels[i].item()], fontsize=12, ha='center')\n",
    "                axs[images_shown, 2].axis('off')\n",
    "                axs[images_shown, 2].set_title(\"Actual\")\n",
    "\n",
    "                images_shown += 1\n",
    "\n",
    "            if images_shown >= num_images:\n",
    "                break\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.95)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def main():\n",
    "    # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    \n",
    "    # model = CNN(config)\n",
    "        \n",
    "    # normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "    #                             std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    # # Dataloaders\n",
    "    # train_transform = transforms.Compose([\n",
    "    #     transforms.RandomResizedCrop(config['image_size'], scale=(0.5, 1.0)),\n",
    "    #     transforms.RandomHorizontalFlip(p=0.5),\n",
    "    #     transforms.RandomRotation(degrees=15),\n",
    "    #     transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.05),\n",
    "    #     transforms.RandomGrayscale(p=0.1),\n",
    "    #     transforms.GaussianBlur(kernel_size=3),\n",
    "    #     transforms.ToTensor(),\n",
    "    #     normalize\n",
    "    # ]) if config['use_augmentation'] else transforms.Compose([\n",
    "    #     transforms.Resize(config['image_size']),\n",
    "    #     transforms.ToTensor(),\n",
    "    #     normalize\n",
    "    # ])\n",
    "\n",
    "    # val_transform = transforms.Compose([\n",
    "    #     transforms.Resize(config['image_size']),\n",
    "    #     transforms.ToTensor(),\n",
    "    #     normalize\n",
    "    # ])\n",
    "\n",
    "\n",
    "    # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # torch.cuda.set_device(device)\n",
    "    # train_dataset = datasets.ImageFolder('train', transform=train_transform)\n",
    "    # val_dataset = datasets.ImageFolder('val', transform=val_transform)\n",
    "    # train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True,num_workers=4, pin_memory=True)\n",
    "    # val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=True,num_workers=4, pin_memory=True)\n",
    "\n",
    "    # trainer = TrainAndPredict(model, device, train_dataset.classes,optimizer=config['optimizer'],lr=config['learning_rate'])\n",
    "\n",
    "    # # Train and log\n",
    "    # trainer.train(train_loader, val_loader, epochs=config['epochs'])\n",
    "    \n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "    test_dataset = datasets.ImageFolder(root='test', transform=transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "    \n",
    "    model = CNN(config).to(device)\n",
    "    model.load_state_dict(torch.load(\"best_model_cnn_final.pth\", map_location=device))\n",
    "\n",
    "    \n",
    "    evaluate_on_test(model, test_loader, device,test_dataset.classes)\n",
    "    \n",
    "    show_predictions_grid(model, test_loader, device, test_dataset.classes, um_images=10)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f310eec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
