{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eb6587d",
   "metadata": {},
   "source": [
    "Question 1 (5 Marks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a6353ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package           Version\n",
      "----------------- -----------\n",
      "asttokens         3.0.0\n",
      "comm              0.2.2\n",
      "debugpy           1.8.14\n",
      "decorator         5.2.1\n",
      "exceptiongroup    1.2.2\n",
      "executing         2.2.0\n",
      "ipykernel         6.29.5\n",
      "ipython           8.35.0\n",
      "jedi              0.19.2\n",
      "jupyter_client    8.6.3\n",
      "jupyter_core      5.7.2\n",
      "matplotlib-inline 0.1.7\n",
      "nest-asyncio      1.6.0\n",
      "packaging         24.2\n",
      "parso             0.8.4\n",
      "pexpect           4.9.0\n",
      "pip               23.0.1\n",
      "platformdirs      4.3.7\n",
      "prompt_toolkit    3.0.50\n",
      "psutil            7.0.0\n",
      "ptyprocess        0.7.0\n",
      "pure_eval         0.2.3\n",
      "Pygments          2.19.1\n",
      "python-dateutil   2.9.0.post0\n",
      "pyzmq             26.4.0\n",
      "setuptools        65.5.0\n",
      "six               1.17.0\n",
      "stack-data        0.6.3\n",
      "tornado           6.4.2\n",
      "traitlets         5.14.3\n",
      "typing_extensions 4.13.2\n",
      "wcwidth           0.2.13\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e896dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/torch/_subclasses/functional_tensor.py:275: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.4345, -0.3812, -0.8483,  ..., -1.1933,  1.0156, -0.9892],\n",
      "          [-1.0213,  0.3691, -0.5498,  ..., -0.7659,  0.5248,  0.8751],\n",
      "          [-1.3958, -0.8850,  0.7639,  ..., -0.0196,  0.6186, -1.0443],\n",
      "          ...,\n",
      "          [ 0.3500, -1.0127,  1.3175,  ..., -0.1168, -1.1132, -2.1547],\n",
      "          [ 1.8374,  1.7425,  0.9138,  ...,  0.3981, -1.4344, -0.0211],\n",
      "          [ 2.8739, -0.0033,  0.9340,  ...,  1.2583,  0.7334,  1.1437]],\n",
      "\n",
      "         [[-0.0101, -0.8702,  0.8240,  ...,  0.5718,  1.4387, -0.5587],\n",
      "          [ 0.6007,  0.4935, -0.0058,  ..., -1.4167, -0.3241,  0.5273],\n",
      "          [ 0.1731, -0.0257,  1.3791,  ..., -1.1895,  0.2785,  0.8173],\n",
      "          ...,\n",
      "          [-0.4200,  1.3456,  0.5558,  ...,  0.3346, -0.4340,  0.4964],\n",
      "          [ 1.2490,  1.5329, -1.4666,  ..., -0.0402,  1.1058,  0.2795],\n",
      "          [ 0.1511,  0.4524, -1.7334,  ...,  0.8952,  0.5167, -0.3186]],\n",
      "\n",
      "         [[-0.3119,  0.0283, -0.2561,  ...,  0.9748, -0.0306, -0.7446],\n",
      "          [-0.1050,  0.5038,  0.7893,  ..., -0.7026,  0.0799,  2.1198],\n",
      "          [ 1.3781, -1.2964,  0.3546,  ...,  1.1915,  0.9963, -0.4069],\n",
      "          ...,\n",
      "          [-0.1857,  0.4656, -0.3730,  ..., -0.1523, -0.2538,  0.0194],\n",
      "          [ 1.4332, -0.2093,  0.3800,  ...,  0.2391, -1.3959, -0.3015],\n",
      "          [ 0.2496, -0.1301, -0.7522,  ...,  1.6049, -0.4854, -0.0563]]]])\n",
      "Output shape: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 channels=3,\n",
    "                 num_filters=64,\n",
    "                 kernel_size=3,\n",
    "                 activation_fn=nn.ReLU,\n",
    "                 dense_neurons=256,\n",
    "                 num_classes=10):\n",
    "        super(CustomCNN, self).__init__()\n",
    "\n",
    "        self.activation_fn = activation_fn\n",
    "\n",
    "        layers = []\n",
    "        current_channels = channels\n",
    "        for _ in range(5):\n",
    "            layers.append(nn.Conv2d(current_channels, num_filters, kernel_size=kernel_size, padding=kernel_size // 2))\n",
    "            layers.append(activation_fn())\n",
    "            layers.append(nn.MaxPool2d(kernel_size=2))\n",
    "            current_channels = num_filters\n",
    "\n",
    "        self.conv_layers = nn.Sequential(*layers)\n",
    "\n",
    "        # ✅ Corrected this line\n",
    "        self.flattened_size = self._get_flattened_size(channels)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.flattened_size, dense_neurons)\n",
    "        self.fc2 = nn.Linear(dense_neurons, num_classes)\n",
    "\n",
    "    def _get_flattened_size(self, in_channels):\n",
    "        dummy_input = torch.zeros(1, in_channels, 224, 224)\n",
    "        with torch.no_grad():\n",
    "            output = self.conv_layers(dummy_input)\n",
    "        return output.view(1, -1).shape[1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.activation_fn()(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Dummy input (simulating a 224x224 RGB image)\n",
    "dummy_image = torch.randn(1, 3, 224, 224)\n",
    "print(dummy_image)\n",
    "\n",
    "# ✅ Instantiate the model\n",
    "model = CustomCNN(\n",
    "    channels=3,\n",
    "    num_filters=64,\n",
    "    kernel_size=3,\n",
    "    activation_fn=nn.ReLU,\n",
    "    dense_neurons=256,\n",
    "    num_classes=10\n",
    ")\n",
    "\n",
    "# ✅ Forward pass\n",
    "output = model(dummy_image)\n",
    "print(\"Output shape:\", output.shape)  # Output shape: torch.Size([1, 10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54784e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransforms\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ImageFolder\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader, Subset\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ✅ Transform (match your model input size)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# ✅ Load dataset (folder structure must be class-wise)\n",
    "dataset_path = 'naturalist_dataset'  # <- your dataset folder\n",
    "dataset = ImageFolder(root=dataset_path, transform=transform)\n",
    "class_names = dataset.classes\n",
    "\n",
    "# ✅ Take only a few samples per class\n",
    "samples_per_class = 2\n",
    "indices = []\n",
    "class_counter = {cls_idx: 0 for cls_idx in range(len(class_names))}\n",
    "\n",
    "for idx, (_, label) in enumerate(dataset):\n",
    "    if class_counter[label] < samples_per_class:\n",
    "        indices.append(idx)\n",
    "        class_counter[label] += 1\n",
    "    if all(v >= samples_per_class for v in class_counter.values()):\n",
    "        break\n",
    "\n",
    "# ✅ Wrap in DataLoader\n",
    "subset = Subset(dataset, indices)\n",
    "dataloader = DataLoader(subset, batch_size=8, shuffle=False)\n",
    "\n",
    "# ✅ Load your model\n",
    "model = model.to('cuda')\n",
    "images = images.to('cuda')\n",
    "model = CustomCNN(num_classes=len(class_names))\n",
    "model.load_state_dict(torch.load('your_model_weights.pth', map_location='cpu'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4ec602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions(model, dataloader, class_names):\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for i in range(images.size(0)):\n",
    "                img = images[i].permute(1, 2, 0).numpy()\n",
    "                true_label = class_names[labels[i]]\n",
    "                predicted_label = class_names[preds[i]]\n",
    "\n",
    "                plt.imshow(img)\n",
    "                plt.title(f\"Predicted: {predicted_label}\\nActual: {true_label}\")\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "\n",
    "# ✅ Show results\n",
    "show_predictions(model, dataloader, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e581f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d198d6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
