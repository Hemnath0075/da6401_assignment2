{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eb6587d",
   "metadata": {},
   "source": [
    "Question 1 (5 Marks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a6353ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                  Version\n",
      "------------------------ -----------\n",
      "asttokens                3.0.0\n",
      "comm                     0.2.2\n",
      "contourpy                1.3.1\n",
      "cycler                   0.12.1\n",
      "debugpy                  1.8.14\n",
      "decorator                5.2.1\n",
      "exceptiongroup           1.2.2\n",
      "executing                2.2.0\n",
      "filelock                 3.18.0\n",
      "fonttools                4.57.0\n",
      "fsspec                   2025.3.2\n",
      "ipykernel                6.29.5\n",
      "ipython                  8.35.0\n",
      "jedi                     0.19.2\n",
      "Jinja2                   3.1.6\n",
      "jupyter_client           8.6.3\n",
      "jupyter_core             5.7.2\n",
      "kiwisolver               1.4.8\n",
      "MarkupSafe               3.0.2\n",
      "matplotlib               3.10.1\n",
      "matplotlib-inline        0.1.7\n",
      "mpmath                   1.3.0\n",
      "nest-asyncio             1.6.0\n",
      "networkx                 3.4.2\n",
      "numpy                    2.2.4\n",
      "nvidia-cublas-cu12       12.4.5.8\n",
      "nvidia-cuda-cupti-cu12   12.4.127\n",
      "nvidia-cuda-nvrtc-cu12   12.4.127\n",
      "nvidia-cuda-runtime-cu12 12.4.127\n",
      "nvidia-cudnn-cu12        9.1.0.70\n",
      "nvidia-cufft-cu12        11.2.1.3\n",
      "nvidia-curand-cu12       10.3.5.147\n",
      "nvidia-cusolver-cu12     11.6.1.9\n",
      "nvidia-cusparse-cu12     12.3.1.170\n",
      "nvidia-cusparselt-cu12   0.6.2\n",
      "nvidia-nccl-cu12         2.21.5\n",
      "nvidia-nvjitlink-cu12    12.4.127\n",
      "nvidia-nvtx-cu12         12.4.127\n",
      "packaging                24.2\n",
      "parso                    0.8.4\n",
      "pexpect                  4.9.0\n",
      "pillow                   11.2.1\n",
      "pip                      23.0.1\n",
      "platformdirs             4.3.7\n",
      "prompt_toolkit           3.0.50\n",
      "psutil                   7.0.0\n",
      "ptyprocess               0.7.0\n",
      "pure_eval                0.2.3\n",
      "Pygments                 2.19.1\n",
      "pyparsing                3.2.3\n",
      "python-dateutil          2.9.0.post0\n",
      "pyzmq                    26.4.0\n",
      "setuptools               65.5.0\n",
      "six                      1.17.0\n",
      "stack-data               0.6.3\n",
      "sympy                    1.13.1\n",
      "torch                    2.6.0\n",
      "torchvision              0.21.0\n",
      "tornado                  6.4.2\n",
      "tqdm                     4.67.1\n",
      "traitlets                5.14.3\n",
      "triton                   3.2.0\n",
      "typing_extensions        4.13.2\n",
      "wcwidth                  0.2.13\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef26318",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1682de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 class folders: ['.DS_Store', 'Aves', 'Insecta', 'Animalia', 'Mammalia', 'Plantae', 'Fungi', 'Amphibia', 'Arachnida', 'Mollusca', 'Reptilia']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing classes:   0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing classes:  18%|█▊        | 2/11 [00:00<00:03,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing classes:  27%|██▋       | 3/11 [00:01<00:04,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing classes:  36%|███▋      | 4/11 [00:02<00:05,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing classes:  45%|████▌     | 5/11 [00:03<00:04,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing classes:  55%|█████▍    | 6/11 [00:04<00:04,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "799\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing classes:  64%|██████▎   | 7/11 [00:05<00:03,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing classes:  73%|███████▎  | 8/11 [00:06<00:02,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing classes:  82%|████████▏ | 9/11 [00:07<00:01,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing classes:  91%|█████████ | 10/11 [00:08<00:01,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing classes: 100%|██████████| 11/11 [00:09<00:00,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is splitted into train of 80% and validation of 20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def split_dataset(\n",
    "    src_dir,\n",
    "    dest_train_dir='train',\n",
    "    dest_val_dir='val',\n",
    "    train_ratio=0.8,\n",
    "    seed=42\n",
    "):\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Create destination directories if they don't exist\n",
    "    os.makedirs(dest_train_dir, exist_ok=True)\n",
    "    os.makedirs(dest_val_dir, exist_ok=True)\n",
    "\n",
    "    classes = os.listdir(src_dir)\n",
    "    print(f\"Found {len(classes)} class folders: {classes}\")\n",
    "\n",
    "    for cls in tqdm(classes, desc=\"Processing classes\"):\n",
    "        src_cls_path = os.path.join(src_dir, cls)\n",
    "        if not os.path.isdir(src_cls_path):\n",
    "            continue\n",
    "\n",
    "        all_images = [img for img in os.listdir(src_cls_path) if img.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        random.shuffle(all_images)\n",
    "\n",
    "        total_images = len(all_images)\n",
    "        train_count = int(train_ratio * total_images)\n",
    "        val_count = total_images - train_count\n",
    "        print(train_count)\n",
    "        print(val_count)\n",
    "\n",
    "        train_images = all_images[:train_count]\n",
    "        val_images = all_images[train_count:]\n",
    "\n",
    "        train_cls_path = os.path.join(dest_train_dir, cls)\n",
    "        val_cls_path = os.path.join(dest_val_dir, cls)\n",
    "\n",
    "        os.makedirs(train_cls_path, exist_ok=True)\n",
    "        os.makedirs(val_cls_path, exist_ok=True)\n",
    "\n",
    "        for img in train_images:\n",
    "            shutil.copy(os.path.join(src_cls_path, img), os.path.join(train_cls_path, img))\n",
    "\n",
    "        for img in val_images:\n",
    "            shutil.copy(os.path.join(src_cls_path, img), os.path.join(val_cls_path, img))\n",
    "\n",
    "    print(\"Dataset is splitted into train of 80% and validation of 20%\")\n",
    "\n",
    "# Usage\n",
    "split_dataset(src_dir=\"inaturalist_12K/train\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2528f2",
   "metadata": {},
   "source": [
    "Question 1 (5 Marks)\n",
    "Build a small CNN model consisting of 5 convolution layers. Each convolution layer would be followed by an activation and a max-pooling layer.\n",
    "\n",
    "After 5 such conv-activation-maxpool blocks, you should have one dense layer followed by the output layer containing 10 neurons (1 for each of the 10 classes). The input layer should be compatible with the images in the iNaturalist dataset dataset.\n",
    "\n",
    "The code should be flexible such that the number of filters, size of filters, and activation function of the convolution layers and dense layers can be changed. You should also be able to change the number of neurons in the dense layer.\n",
    "\n",
    "What is the total number of computations done by your network? (assume mmm filters in each layer of size k×kk\\times kk×k and nnn neurons in the dense layer)\n",
    "What is the total number of parameters in your network? (assume mmm filters in each layer of size k×kk\\times kk×k and nnn neurons in the dense layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e896dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(SmallCNN, self).__init__()\n",
    "\n",
    "        in_channels = 3  # RGB\n",
    "        conv_layers = []\n",
    "\n",
    "        for i in range(5):\n",
    "            conv_layers.append(nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=config['conv_filters'][i],\n",
    "                kernel_size=config['kernel_sizes'][i],\n",
    "                padding=1\n",
    "            ))\n",
    "            conv_layers.append(config['conv_activation']())\n",
    "            conv_layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            in_channels = config['conv_filters'][i]\n",
    "\n",
    "        self.conv_block = nn.Sequential(*conv_layers)\n",
    "\n",
    "        dummy_input = torch.zeros(1, 3, *config['image_size'])\n",
    "        dummy_output = self.conv_block(dummy_input)\n",
    "        flat_size = dummy_output.view(1, -1).shape[1]\n",
    "\n",
    "        self.fc1 = nn.Linear(flat_size, config['dense_neurons'])\n",
    "        self.act_dense = config['dense_activation']()\n",
    "        self.output = nn.Linear(config['dense_neurons'], config['num_classes'])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.act_dense(self.fc1(x))\n",
    "        return self.output(x)\n",
    "    \n",
    "config = {\n",
    "    'conv_filters': [32, 64, 128, 256, 256],\n",
    "    'kernel_sizes': [3, 3, 3, 3, 3],\n",
    "    'conv_activation': nn.ReLU,\n",
    "    'dense_activation': nn.ReLU,\n",
    "    'dense_neurons': 512,\n",
    "    'num_classes': 10,\n",
    "    'image_size': (128, 128)\n",
    "}\n",
    "\n",
    "model = SmallCNN(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f17effa",
   "metadata": {},
   "source": [
    "Layer 1: ConvBlock1 | In: 3 | Out: 32 | Kernel: 3 | Activation: ReLU | Pooling: MaxPool2d(2x2)\n",
    "\n",
    "Layer 2: ConvBlock2 | In: 32 | Out: 64 | Kernel: 3 | Activation: ReLU | Pooling: MaxPool2d(2x2)\n",
    "\n",
    "Layer 3: ConvBlock3 | In: 64 | Out: 128 | Kernel: 3 | Activation: ReLU | Pooling: MaxPool2d(2x2)\n",
    "\n",
    "Layer 4: ConvBlock4 | In: 128 | Out: 256 | Kernel: 3 | Activation: ReLU | Pooling: MaxPool2d(2x2)\n",
    "\n",
    "Layer 5: ConvBlock5 | In: 256 | Out: 256 | Kernel: 3 | Activation: ReLU | Pooling: MaxPool2d(2x2)\n",
    "\n",
    "Dense Layer: 512 neurons | Activation: ReLU\n",
    "\n",
    "Output Layer: 10 neurons for 10 classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54784e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class TrainerPredictor:\n",
    "    def __init__(self, model, device, class_names, lr=0.001):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.class_names = class_names\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "    def train(self, train_loader, val_loader, epochs=10):\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "\n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            val_acc = self.validate(val_loader)\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | Loss: {total_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    def validate(self, val_loader):\n",
    "        self.model.eval()\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(images)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        return 100 * correct / total\n",
    "\n",
    "    def predict(self, image_tensor):\n",
    "        self.model.eval()\n",
    "        image_tensor = image_tensor.to(self.device).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(image_tensor)\n",
    "            _, pred = torch.max(outputs, 1)\n",
    "        \n",
    "        return self.class_names[pred.item()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a22438f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d4ec602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Loss: 287.6734 | Val Acc: 13.20%\n",
      "Epoch 2/10 | Loss: 277.8231 | Val Acc: 20.45%\n",
      "Epoch 3/10 | Loss: 263.9331 | Val Acc: 26.15%\n",
      "Epoch 4/10 | Loss: 254.9120 | Val Acc: 27.15%\n",
      "Epoch 5/10 | Loss: 245.4232 | Val Acc: 28.55%\n",
      "Epoch 6/10 | Loss: 238.8246 | Val Acc: 28.50%\n",
      "Epoch 7/10 | Loss: 229.0875 | Val Acc: 32.45%\n",
      "Epoch 8/10 | Loss: 218.4348 | Val Acc: 31.50%\n",
      "Epoch 9/10 | Loss: 207.5130 | Val Acc: 30.30%\n",
      "Epoch 10/10 | Loss: 190.5046 | Val Acc: 31.15%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    'conv_filters': [32, 64, 128, 256, 256],\n",
    "    'kernel_sizes': [3, 3, 3, 3, 3],\n",
    "    'conv_activation': nn.ReLU,\n",
    "    'dense_activation': nn.ReLU,\n",
    "    'dense_neurons': 512,\n",
    "    'num_classes': 10,\n",
    "    'image_size': (128, 128)\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load data\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(config['image_size']),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder('train', transform=transform)\n",
    "val_dataset = datasets.ImageFolder('val', transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize\n",
    "model = SmallCNN(config)\n",
    "trainer = TrainerPredictor(model, device, train_dataset.classes)\n",
    "\n",
    "# Train\n",
    "trainer.train(train_loader, val_loader, epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddc1cb8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a468068c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Loss: 293.8017 | Val Acc: 15.38%\n",
      "Epoch 2/10 | Loss: 286.5302 | Val Acc: 20.03%\n",
      "Epoch 3/10 | Loss: 280.2457 | Val Acc: 25.00%\n",
      "Epoch 4/10 | Loss: 275.7233 | Val Acc: 26.27%\n",
      "Epoch 5/10 | Loss: 269.8967 | Val Acc: 25.52%\n",
      "Epoch 6/10 | Loss: 266.1672 | Val Acc: 26.03%\n",
      "Epoch 7/10 | Loss: 266.0421 | Val Acc: 27.86%\n",
      "Epoch 8/10 | Loss: 263.0377 | Val Acc: 30.49%\n",
      "Epoch 9/10 | Loss: 261.2026 | Val Acc: 31.52%\n",
      "Epoch 10/10 | Loss: 258.7496 | Val Acc: 31.52%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    'conv_filters': [32, 64, 128, 256, 256],\n",
    "    'kernel_sizes': [3, 3, 3, 3, 3],\n",
    "    'conv_activation': nn.ReLU,\n",
    "    'dense_activation': nn.ReLU,\n",
    "    'dense_neurons': 512,\n",
    "    'num_classes': 10,\n",
    "    'image_size': (700, 700)\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(config['image_size'], scale=(0.7, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.05),\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.GaussianBlur(kernel_size=3),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "# For validation, keep only resizing and tensor conversion (no augmentation)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(config['image_size']),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = datasets.ImageFolder('train', transform=train_transform)\n",
    "val_dataset = datasets.ImageFolder('val', transform=val_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize\n",
    "model = SmallCNN(config)\n",
    "trainer = TrainerPredictor(model, device, train_dataset.classes)\n",
    "\n",
    "# Train\n",
    "trainer.train(train_loader, val_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e581f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Loss: 633.0022 | Val Acc: 20.59%\n",
      "Epoch 2/10 | Loss: 577.2128 | Val Acc: 22.09%\n",
      "Epoch 3/10 | Loss: 573.8492 | Val Acc: 22.28%\n",
      "Epoch 4/10 | Loss: 572.3456 | Val Acc: 19.84%\n",
      "Epoch 5/10 | Loss: 571.4403 | Val Acc: 23.83%\n",
      "Epoch 6/10 | Loss: 570.1455 | Val Acc: 20.83%\n",
      "Epoch 7/10 | Loss: 567.7567 | Val Acc: 23.45%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "\n",
    "activations = {\n",
    "    'relu': nn.ReLU(),\n",
    "    'tanh': nn.Tanh(),\n",
    "    'sigmoid': nn.Sigmoid(),\n",
    "    'leaky_relu': nn.LeakyReLU(),\n",
    "}\n",
    "\n",
    "config = {\n",
    "    'input_dimension': (3, 250, 250),  # Adjusted to (500, 500)\n",
    "    'conv_filters': [32, 64, 128, 256, 256],  # For 5 conv layers\n",
    "    'kernel_sizes': [3, 3, 3, 3, 3],\n",
    "    'stride': 1,\n",
    "    'padding': 1,\n",
    "    'max_pooling_size': 2,\n",
    "    'dropout_rate': 0.25,\n",
    "    'use_batchnorm': True,\n",
    "    'factor': 1.0,\n",
    "    'dropout_organisation': 'after_relu',  # or 'before_relu'\n",
    "    'dense_neurons': 512,\n",
    "    'num_classes': 10,\n",
    "    'conv_activation': 'relu',\n",
    "    'dense_activation': 'relu',\n",
    "    'image_size': (250, 250),\n",
    "}\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        in_channels = config['input_dimension'][0]\n",
    "        conv_filters = config['conv_filters']\n",
    "        kernel_sizes = config['kernel_sizes']\n",
    "        stride = config['stride']\n",
    "        padding = config['padding']\n",
    "        pool = config['max_pooling_size']\n",
    "        dropout = config['dropout_rate']\n",
    "        use_bn = config['use_batchnorm']\n",
    "        factor = config['factor']\n",
    "        dropout_org = config['dropout_organisation']\n",
    "\n",
    "        conv_layers = []\n",
    "        for i in range(5):  # Hardcoded 5 conv layers\n",
    "            out_channels = int(conv_filters[i] * factor)\n",
    "            conv_layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_sizes[i], stride=stride, padding=padding))\n",
    "            if use_bn:\n",
    "                conv_layers.append(nn.BatchNorm2d(out_channels))\n",
    "            if dropout_org == 'before_relu':\n",
    "                conv_layers.append(nn.Dropout2d(dropout))\n",
    "            conv_layers.append(activations[config['conv_activation']])\n",
    "            if dropout_org == 'after_relu':\n",
    "                conv_layers.append(nn.Dropout2d(dropout))\n",
    "            conv_layers.append(nn.MaxPool2d(kernel_size=pool))\n",
    "            in_channels = out_channels\n",
    "\n",
    "        self.conv = nn.Sequential(*conv_layers)\n",
    "\n",
    "        # Estimate flattened size\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros((1, *config['input_dimension']))\n",
    "            dummy_output = self.conv(dummy_input)\n",
    "            flattened_size = dummy_output.view(1, -1).shape[1]\n",
    "\n",
    "        # The fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(flattened_size, config['dense_neurons']),\n",
    "            activations[config['dense_activation']],\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(config['dense_neurons'], config['num_classes'])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output from the conv layers\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# Now, just pass the config dictionary to the CNN class\n",
    "model = CNN(config)  # Instead of passing each parameter individually\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(config['image_size'], scale=(0.5, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.05),\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.GaussianBlur(kernel_size=3),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# For validation, keep only resizing and tensor conversion (no augmentation)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(config['image_size']),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder('train', transform=train_transform)\n",
    "val_dataset = datasets.ImageFolder('val', transform=val_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "trainer = TrainerPredictor(model, device, train_dataset.classes)\n",
    "\n",
    "# Train\n",
    "trainer.train(train_loader, val_loader, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d198d6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f00f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
