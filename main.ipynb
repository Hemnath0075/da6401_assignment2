{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eb6587d",
   "metadata": {},
   "source": [
    "Question 1 (5 Marks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a6353ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                  Version\n",
      "------------------------ -----------\n",
      "annotated-types          0.7.0\n",
      "asttokens                3.0.0\n",
      "certifi                  2025.1.31\n",
      "charset-normalizer       3.4.1\n",
      "click                    8.1.8\n",
      "comm                     0.2.2\n",
      "contourpy                1.3.1\n",
      "cycler                   0.12.1\n",
      "debugpy                  1.8.14\n",
      "decorator                5.2.1\n",
      "docker-pycreds           0.4.0\n",
      "exceptiongroup           1.2.2\n",
      "executing                2.2.0\n",
      "filelock                 3.18.0\n",
      "fonttools                4.57.0\n",
      "fsspec                   2025.3.2\n",
      "gitdb                    4.0.12\n",
      "GitPython                3.1.44\n",
      "idna                     3.10\n",
      "ipykernel                6.29.5\n",
      "ipython                  8.35.0\n",
      "jedi                     0.19.2\n",
      "Jinja2                   3.1.6\n",
      "jupyter_client           8.6.3\n",
      "jupyter_core             5.7.2\n",
      "kiwisolver               1.4.8\n",
      "MarkupSafe               3.0.2\n",
      "matplotlib               3.10.1\n",
      "matplotlib-inline        0.1.7\n",
      "mpmath                   1.3.0\n",
      "nest-asyncio             1.6.0\n",
      "networkx                 3.4.2\n",
      "numpy                    2.2.4\n",
      "nvidia-cublas-cu12       12.4.5.8\n",
      "nvidia-cuda-cupti-cu12   12.4.127\n",
      "nvidia-cuda-nvrtc-cu12   12.4.127\n",
      "nvidia-cuda-runtime-cu12 12.4.127\n",
      "nvidia-cudnn-cu12        9.1.0.70\n",
      "nvidia-cufft-cu12        11.2.1.3\n",
      "nvidia-curand-cu12       10.3.5.147\n",
      "nvidia-cusolver-cu12     11.6.1.9\n",
      "nvidia-cusparse-cu12     12.3.1.170\n",
      "nvidia-cusparselt-cu12   0.6.2\n",
      "nvidia-nccl-cu12         2.21.5\n",
      "nvidia-nvjitlink-cu12    12.4.127\n",
      "nvidia-nvtx-cu12         12.4.127\n",
      "packaging                24.2\n",
      "parso                    0.8.4\n",
      "pexpect                  4.9.0\n",
      "pillow                   11.2.1\n",
      "pip                      23.0.1\n",
      "platformdirs             4.3.7\n",
      "prompt_toolkit           3.0.50\n",
      "protobuf                 5.29.4\n",
      "psutil                   7.0.0\n",
      "ptyprocess               0.7.0\n",
      "pure_eval                0.2.3\n",
      "pydantic                 2.11.3\n",
      "pydantic_core            2.33.1\n",
      "Pygments                 2.19.1\n",
      "pyparsing                3.2.3\n",
      "python-dateutil          2.9.0.post0\n",
      "PyYAML                   6.0.2\n",
      "pyzmq                    26.4.0\n",
      "requests                 2.32.3\n",
      "sentry-sdk               2.26.1\n",
      "setproctitle             1.3.5\n",
      "setuptools               65.5.0\n",
      "six                      1.17.0\n",
      "smmap                    5.0.2\n",
      "stack-data               0.6.3\n",
      "sympy                    1.13.1\n",
      "torch                    2.6.0\n",
      "torchvision              0.21.0\n",
      "tornado                  6.4.2\n",
      "tqdm                     4.67.1\n",
      "traitlets                5.14.3\n",
      "triton                   3.2.0\n",
      "typing_extensions        4.13.2\n",
      "typing-inspection        0.4.0\n",
      "urllib3                  2.4.0\n",
      "wandb                    0.19.9\n",
      "wcwidth                  0.2.13\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: wandb in ./venv/lib/python3.10/site-packages (0.19.9)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in ./venv/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in ./venv/lib/python3.10/site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in ./venv/lib/python3.10/site-packages (from wandb) (5.29.4)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in ./venv/lib/python3.10/site-packages (from wandb) (2.26.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in ./venv/lib/python3.10/site-packages (from wandb) (4.13.2)\n",
      "Requirement already satisfied: platformdirs in ./venv/lib/python3.10/site-packages (from wandb) (4.3.7)\n",
      "Requirement already satisfied: pyyaml in ./venv/lib/python3.10/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in ./venv/lib/python3.10/site-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.10/site-packages (from wandb) (65.5.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in ./venv/lib/python3.10/site-packages (from wandb) (7.0.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in ./venv/lib/python3.10/site-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: pydantic<3 in ./venv/lib/python3.10/site-packages (from wandb) (2.11.3)\n",
      "Requirement already satisfied: setproctitle in ./venv/lib/python3.10/site-packages (from wandb) (1.3.5)\n",
      "Requirement already satisfied: six>=1.4.0 in ./venv/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in ./venv/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in ./venv/lib/python3.10/site-packages (from pydantic<3->wandb) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./venv/lib/python3.10/site-packages (from pydantic<3->wandb) (0.4.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.10/site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in ./venv/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip list\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef26318",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1682de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 class folders: ['.DS_Store', 'Aves', 'Insecta', 'Animalia', 'Mammalia', 'Plantae', 'Fungi', 'Amphibia', 'Arachnida', 'Mollusca', 'Reptilia']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing classes:   0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing classes:  18%|█▊        | 2/11 [00:00<00:03,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing classes:  27%|██▋       | 3/11 [00:01<00:04,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing classes:  36%|███▋      | 4/11 [00:02<00:05,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing classes:  45%|████▌     | 5/11 [00:03<00:04,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing classes:  55%|█████▍    | 6/11 [00:04<00:04,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "799\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing classes:  64%|██████▎   | 7/11 [00:05<00:03,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing classes:  73%|███████▎  | 8/11 [00:06<00:02,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing classes:  82%|████████▏ | 9/11 [00:07<00:01,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing classes:  91%|█████████ | 10/11 [00:08<00:01,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing classes: 100%|██████████| 11/11 [00:09<00:00,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is splitted into train of 80% and validation of 20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def split_dataset(\n",
    "    src_dir,\n",
    "    dest_train_dir='train',\n",
    "    dest_val_dir='val',\n",
    "    train_ratio=0.8,\n",
    "    seed=42\n",
    "):\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Create destination directories if they don't exist\n",
    "    os.makedirs(dest_train_dir, exist_ok=True)\n",
    "    os.makedirs(dest_val_dir, exist_ok=True)\n",
    "\n",
    "    classes = os.listdir(src_dir)\n",
    "    print(f\"Found {len(classes)} class folders: {classes}\")\n",
    "\n",
    "    for cls in tqdm(classes, desc=\"Processing classes\"):\n",
    "        src_cls_path = os.path.join(src_dir, cls)\n",
    "        if not os.path.isdir(src_cls_path):\n",
    "            continue\n",
    "\n",
    "        all_images = [img for img in os.listdir(src_cls_path) if img.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        random.shuffle(all_images)\n",
    "\n",
    "        total_images = len(all_images)\n",
    "        train_count = int(train_ratio * total_images)\n",
    "        val_count = total_images - train_count\n",
    "        print(train_count)\n",
    "        print(val_count)\n",
    "\n",
    "        train_images = all_images[:train_count]\n",
    "        val_images = all_images[train_count:]\n",
    "\n",
    "        train_cls_path = os.path.join(dest_train_dir, cls)\n",
    "        val_cls_path = os.path.join(dest_val_dir, cls)\n",
    "\n",
    "        os.makedirs(train_cls_path, exist_ok=True)\n",
    "        os.makedirs(val_cls_path, exist_ok=True)\n",
    "\n",
    "        for img in train_images:\n",
    "            shutil.copy(os.path.join(src_cls_path, img), os.path.join(train_cls_path, img))\n",
    "\n",
    "        for img in val_images:\n",
    "            shutil.copy(os.path.join(src_cls_path, img), os.path.join(val_cls_path, img))\n",
    "\n",
    "    print(\"Dataset is splitted into train of 80% and validation of 20%\")\n",
    "\n",
    "# Usage\n",
    "split_dataset(src_dir=\"inaturalist_12K/train\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2528f2",
   "metadata": {},
   "source": [
    "Question 1 (5 Marks)\n",
    "Build a small CNN model consisting of 5 convolution layers. Each convolution layer would be followed by an activation and a max-pooling layer.\n",
    "\n",
    "After 5 such conv-activation-maxpool blocks, you should have one dense layer followed by the output layer containing 10 neurons (1 for each of the 10 classes). The input layer should be compatible with the images in the iNaturalist dataset dataset.\n",
    "\n",
    "The code should be flexible such that the number of filters, size of filters, and activation function of the convolution layers and dense layers can be changed. You should also be able to change the number of neurons in the dense layer.\n",
    "\n",
    "What is the total number of computations done by your network? (assume mmm filters in each layer of size k×kk\\times kk×k and nnn neurons in the dense layer)\n",
    "What is the total number of parameters in your network? (assume mmm filters in each layer of size k×kk\\times kk×k and nnn neurons in the dense layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e896dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(SmallCNN, self).__init__()\n",
    "\n",
    "        in_channels = 3  # RGB\n",
    "        conv_layers = []\n",
    "\n",
    "        for i in range(5):\n",
    "            conv_layers.append(nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=config['conv_filters'][i],\n",
    "                kernel_size=config['kernel_sizes'][i],\n",
    "                padding=1\n",
    "            ))\n",
    "            conv_layers.append(config['conv_activation']())\n",
    "            conv_layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            in_channels = config['conv_filters'][i]\n",
    "\n",
    "        self.conv_block = nn.Sequential(*conv_layers)\n",
    "\n",
    "        dummy_input = torch.zeros(1, 3, *config['image_size'])\n",
    "        dummy_output = self.conv_block(dummy_input)\n",
    "        flat_size = dummy_output.view(1, -1).shape[1]\n",
    "\n",
    "        self.fc1 = nn.Linear(flat_size, config['dense_neurons'])\n",
    "        self.act_dense = config['dense_activation']()\n",
    "        self.output = nn.Linear(config['dense_neurons'], config['num_classes'])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.act_dense(self.fc1(x))\n",
    "        return self.output(x)\n",
    "    \n",
    "config = {\n",
    "    'conv_filters': [32, 64, 128, 256, 256],\n",
    "    'kernel_sizes': [3, 3, 3, 3, 3],\n",
    "    'conv_activation': nn.ReLU,\n",
    "    'dense_activation': nn.ReLU,\n",
    "    'dense_neurons': 512,\n",
    "    'num_classes': 10,\n",
    "    'image_size': (128, 128)\n",
    "}\n",
    "\n",
    "model = SmallCNN(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f17effa",
   "metadata": {},
   "source": [
    "Layer 1: ConvBlock1 | In: 3 | Out: 32 | Kernel: 3 | Activation: ReLU | Pooling: MaxPool2d(2x2)\n",
    "\n",
    "Layer 2: ConvBlock2 | In: 32 | Out: 64 | Kernel: 3 | Activation: ReLU | Pooling: MaxPool2d(2x2)\n",
    "\n",
    "Layer 3: ConvBlock3 | In: 64 | Out: 128 | Kernel: 3 | Activation: ReLU | Pooling: MaxPool2d(2x2)\n",
    "\n",
    "Layer 4: ConvBlock4 | In: 128 | Out: 256 | Kernel: 3 | Activation: ReLU | Pooling: MaxPool2d(2x2)\n",
    "\n",
    "Layer 5: ConvBlock5 | In: 256 | Out: 256 | Kernel: 3 | Activation: ReLU | Pooling: MaxPool2d(2x2)\n",
    "\n",
    "Dense Layer: 512 neurons | Activation: ReLU\n",
    "\n",
    "Output Layer: 10 neurons for 10 classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54784e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class TrainAndPredict:\n",
    "    def __init__(self, model, device, class_names, lr=0.001):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.class_names = class_names\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "    def train(self, train_loader, val_loader, epochs=10):\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "\n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            val_acc = self.validate(val_loader)\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | Loss: {total_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    def validate(self, val_loader):\n",
    "        self.model.eval()\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(images)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        return 100 * correct / total\n",
    "\n",
    "    def predict(self, image_tensor):\n",
    "        self.model.eval()\n",
    "        image_tensor = image_tensor.to(self.device).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(image_tensor)\n",
    "            _, pred = torch.max(outputs, 1)\n",
    "        \n",
    "        return self.class_names[pred.item()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a22438f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4ec602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Loss: 287.6734 | Val Acc: 13.20%\n",
      "Epoch 2/10 | Loss: 277.8231 | Val Acc: 20.45%\n",
      "Epoch 3/10 | Loss: 263.9331 | Val Acc: 26.15%\n",
      "Epoch 4/10 | Loss: 254.9120 | Val Acc: 27.15%\n",
      "Epoch 5/10 | Loss: 245.4232 | Val Acc: 28.55%\n",
      "Epoch 6/10 | Loss: 238.8246 | Val Acc: 28.50%\n",
      "Epoch 7/10 | Loss: 229.0875 | Val Acc: 32.45%\n",
      "Epoch 8/10 | Loss: 218.4348 | Val Acc: 31.50%\n",
      "Epoch 9/10 | Loss: 207.5130 | Val Acc: 30.30%\n",
      "Epoch 10/10 | Loss: 190.5046 | Val Acc: 31.15%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    'conv_filters': [32, 64, 128, 256, 256],\n",
    "    'kernel_sizes': [3, 3, 3, 3, 3],\n",
    "    'conv_activation': nn.ReLU,\n",
    "    'dense_activation': nn.ReLU,\n",
    "    'dense_neurons': 512,\n",
    "    'num_classes': 10,\n",
    "    'image_size': (128, 128)\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load data\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(config['image_size']),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder('train', transform=transform)\n",
    "val_dataset = datasets.ImageFolder('val', transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize\n",
    "model = SmallCNN(config)\n",
    "trainer = TrainAndPredict(model, device, train_dataset.classes)\n",
    "\n",
    "# Train\n",
    "trainer.train(train_loader, val_loader, epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddc1cb8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a468068c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Loss: 293.8017 | Val Acc: 15.38%\n",
      "Epoch 2/10 | Loss: 286.5302 | Val Acc: 20.03%\n",
      "Epoch 3/10 | Loss: 280.2457 | Val Acc: 25.00%\n",
      "Epoch 4/10 | Loss: 275.7233 | Val Acc: 26.27%\n",
      "Epoch 5/10 | Loss: 269.8967 | Val Acc: 25.52%\n",
      "Epoch 6/10 | Loss: 266.1672 | Val Acc: 26.03%\n",
      "Epoch 7/10 | Loss: 266.0421 | Val Acc: 27.86%\n",
      "Epoch 8/10 | Loss: 263.0377 | Val Acc: 30.49%\n",
      "Epoch 9/10 | Loss: 261.2026 | Val Acc: 31.52%\n",
      "Epoch 10/10 | Loss: 258.7496 | Val Acc: 31.52%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    'conv_filters': [32, 64, 128, 256, 256],\n",
    "    'kernel_sizes': [3, 3, 3, 3, 3],\n",
    "    'conv_activation': nn.ReLU,\n",
    "    'dense_activation': nn.ReLU,\n",
    "    'dense_neurons': 512,\n",
    "    'num_classes': 10,\n",
    "    'image_size': (700, 700)\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(config['image_size'], scale=(0.7, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.05),\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.GaussianBlur(kernel_size=3),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "# For validation, keep only resizing and tensor conversion (no augmentation)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(config['image_size']),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = datasets.ImageFolder('train', transform=train_transform)\n",
    "val_dataset = datasets.ImageFolder('val', transform=val_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize\n",
    "model = SmallCNN(config)\n",
    "trainer = TrainerPredictor(model, device, train_dataset.classes)\n",
    "\n",
    "# Train\n",
    "trainer.train(train_loader, val_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e581f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f0e54566d40>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f0e90c99ff0, raw_cell=\"import torch\n",
      "import torch.nn as nn\n",
      "import torchvis..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell://ssh-remote%2B10.24.6.107/mnt/e_disk/ch24s016/da6401_assignment2/main.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:552\u001b[0m, in \u001b[0;36m_WandbInit._resume_backend\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39minterface \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresuming backend\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 552\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpublish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/wandb/sdk/interface/interface.py:777\u001b[0m, in \u001b[0;36mInterfaceBase.publish_resume\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpublish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    776\u001b[0m     resume \u001b[38;5;241m=\u001b[39m pb\u001b[38;5;241m.\u001b[39mResumeRequest()\n\u001b[0;32m--> 777\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py:293\u001b[0m, in \u001b[0;36mInterfaceShared._publish_resume\u001b[0;34m(self, resume)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_publish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m, resume: pb\u001b[38;5;241m.\u001b[39mResumeRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    292\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(resume\u001b[38;5;241m=\u001b[39mresume)\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/wandb/sdk/interface/interface_sock.py:39\u001b[0m, in \u001b[0;36mInterfaceSock._publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_publish\u001b[39m(\u001b[38;5;28mself\u001b[39m, record: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpb.Record\u001b[39m\u001b[38;5;124m\"\u001b[39m, local: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign(record)\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_record_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:174\u001b[0m, in \u001b[0;36mSockClient.send_record_publish\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    172\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrequest_id \u001b[38;5;241m=\u001b[39m record\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mmailbox_slot\n\u001b[1;32m    173\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrecord_publish\u001b[38;5;241m.\u001b[39mCopyFrom(record)\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_server_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_req\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:154\u001b[0m, in \u001b[0;36mSockClient.send_server_request\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend_server_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, msg: spb\u001b[38;5;241m.\u001b[39mServerRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:151\u001b[0m, in \u001b[0;36mSockClient._send_message\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    149\u001b[0m header \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<BI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m), raw_size)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendall_with_error_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:130\u001b[0m, in \u001b[0;36mSockClient._sendall_with_error_handle\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    128\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     sent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m# sent equal to 0 indicates a closed socket\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sent \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: di20w0lf\n",
      "Sweep URL: https://wandb.ai/ch24s016-iitm/iNaturalist_CNN/sweeps/di20w0lf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-40 (_run_job):\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 300, in _run_job\n",
      "    wandb.teardown()\n",
      "  File \"/mnt/c_disk/hemnath/.pyenv/versions/3.10.11/lib/python3.10/contextlib.py\", line 79, in inner\n",
      "    return func(*args, **kwds)\n",
      "  File \"/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/wandb/sdk/wandb_setup.py\", line 404, in teardown\n",
      "    orig_singleton._teardown(exit_code=exit_code)\n",
      "  File \"/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/wandb/sdk/wandb_setup.py\", line 249, in _teardown\n",
      "    internal_exit_code = self._connection.teardown(exit_code or 0)\n",
      "  File \"/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/wandb/sdk/lib/service_connection.py\", line 228, in teardown\n",
      "    self._client.send_server_request(\n",
      "  File \"/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py\", line 154, in send_server_request\n",
      "    self._send_message(msg)\n",
      "  File \"/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py\", line 151, in _send_message\n",
      "    self._sendall_with_error_handle(header + data)\n",
      "  File \"/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
      "    sent = self._sock.send(data)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/c_disk/hemnath/.pyenv/versions/3.10.11/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/mnt/c_disk/hemnath/.pyenv/versions/3.10.11/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 311, in _run_job\n",
      "    wandb.finish(exit_code=1)\n",
      "  File \"/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 4238, in finish\n",
      "    wandb.run.finish(exit_code=exit_code, quiet=quiet)\n",
      "  File \"/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 387, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 483, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 425, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 2182, in finish\n",
      "    return self._finish(exit_code)\n",
      "  File \"/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 387, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 2190, in _finish\n",
      "    with telemetry.context(run=self) as tel:\n",
      "  File \"/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/wandb/sdk/lib/telemetry.py\", line 42, in __exit__\n",
      "    self._run._telemetry_callback(self._obj)\n",
      "  File \"/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 783, in _telemetry_callback\n",
      "    self._telemetry_flush()\n",
      "  File \"/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 796, in _telemetry_flush\n",
      "    self._backend.interface._publish_telemetry(self._telemetry_obj)\n",
      "  File \"/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py\", line 60, in _publish_telemetry\n",
      "    self._publish(rec)\n",
      "  File \"/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/wandb/sdk/interface/interface_sock.py\", line 39, in _publish\n",
      "    self._sock_client.send_record_publish(record)\n",
      "  File \"/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py\", line 174, in send_record_publish\n",
      "    self.send_server_request(server_req)\n",
      "  File \"/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py\", line 154, in send_server_request\n",
      "    self._send_message(msg)\n",
      "  File \"/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py\", line 151, in _send_message\n",
      "    self._sendall_with_error_handle(header + data)\n",
      "  File \"/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
      "    sent = self._sock.send(data)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f0e54566d40>> (for post_run_cell), with arguments args (<ExecutionResult object at 7f0e721f8730, execution_count=4 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7f0e90c99ff0, raw_cell=\"import torch\n",
      "import torch.nn as nn\n",
      "import torchvis..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell://ssh-remote%2B10.24.6.107/mnt/e_disk/ch24s016/da6401_assignment2/main.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:547\u001b[0m, in \u001b[0;36m_WandbInit._pause_backend\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39minterface \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpausing backend\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 547\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpublish_pause\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/wandb/sdk/interface/interface.py:769\u001b[0m, in \u001b[0;36mInterfaceBase.publish_pause\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpublish_pause\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    768\u001b[0m     pause \u001b[38;5;241m=\u001b[39m pb\u001b[38;5;241m.\u001b[39mPauseRequest()\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish_pause\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpause\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py:289\u001b[0m, in \u001b[0;36mInterfaceShared._publish_pause\u001b[0;34m(self, pause)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_publish_pause\u001b[39m(\u001b[38;5;28mself\u001b[39m, pause: pb\u001b[38;5;241m.\u001b[39mPauseRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(pause\u001b[38;5;241m=\u001b[39mpause)\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/wandb/sdk/interface/interface_sock.py:39\u001b[0m, in \u001b[0;36mInterfaceSock._publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_publish\u001b[39m(\u001b[38;5;28mself\u001b[39m, record: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpb.Record\u001b[39m\u001b[38;5;124m\"\u001b[39m, local: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign(record)\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_record_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:174\u001b[0m, in \u001b[0;36mSockClient.send_record_publish\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    172\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrequest_id \u001b[38;5;241m=\u001b[39m record\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mmailbox_slot\n\u001b[1;32m    173\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrecord_publish\u001b[38;5;241m.\u001b[39mCopyFrom(record)\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_server_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_req\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:154\u001b[0m, in \u001b[0;36mSockClient.send_server_request\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend_server_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, msg: spb\u001b[38;5;241m.\u001b[39mServerRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:151\u001b[0m, in \u001b[0;36mSockClient._send_message\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    149\u001b[0m header \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<BI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m), raw_size)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendall_with_error_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/e_disk/ch24s016/da6401_assignment2/venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:130\u001b[0m, in \u001b[0;36mSockClient._sendall_with_error_handle\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    128\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     sent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m# sent equal to 0 indicates a closed socket\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sent \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "activations = {\n",
    "    'relu': nn.ReLU(),\n",
    "    'tanh': nn.Tanh(),\n",
    "    'sigmoid': nn.Sigmoid(),\n",
    "    'leaky_relu': nn.LeakyReLU(),\n",
    "    'mish':nn.Mish(),\n",
    "    'gelu':nn.GELU(),\n",
    "    'silu':nn.SiLU(),\n",
    "    'relu6':nn.ReLU6()\n",
    "}\n",
    "\n",
    "optimizer_dict = {\n",
    "    'adam': optim.Adam,\n",
    "    'adamw': optim.AdamW,\n",
    "    'sgd': optim.SGD\n",
    "}\n",
    "\n",
    "\n",
    "def generate_filters(base_m, strategy):\n",
    "            if strategy == 'same':\n",
    "                return [base_m] * 5\n",
    "            elif strategy == 'double':\n",
    "                return [base_m * (2 ** i) for i in range(5)]\n",
    "            elif strategy == 'half':\n",
    "                return [max(1, base_m // (2 ** i)) for i in range(5)]\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        in_channels = config['input_dimension'][0]\n",
    "        base_m = config['conv_filters']\n",
    "        strategy = config['filter_org']\n",
    "        conv_filters = generate_filters(base_m, strategy)\n",
    "        kernel_sizes = config['kernel_sizes']\n",
    "        stride = config['stride']\n",
    "        padding = config['padding']\n",
    "        pool = config['max_pooling_size']\n",
    "        dropout = config['dropout_rate']\n",
    "        use_bn = config['use_batchnorm']\n",
    "        dropout_org = config['dropout_organisation']\n",
    "\n",
    "        conv_layers = []\n",
    "        for i in range(5):  # 5 conv layers\n",
    "            out_channels = conv_filters[i]\n",
    "            conv_layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_sizes[i], stride=stride, padding=padding))\n",
    "            if use_bn:\n",
    "                conv_layers.append(nn.BatchNorm2d(out_channels))\n",
    "            if dropout_org == 'before_relu':\n",
    "                conv_layers.append(nn.Dropout2d(dropout))\n",
    "            conv_layers.append(activations[config['conv_activation']])\n",
    "            if dropout_org == 'after_relu':\n",
    "                conv_layers.append(nn.Dropout2d(dropout))\n",
    "            conv_layers.append(nn.MaxPool2d(kernel_size=pool))\n",
    "            in_channels = out_channels\n",
    "\n",
    "        self.conv = nn.Sequential(*conv_layers)\n",
    "\n",
    "        # Estimate flattened size\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros((1, *config['input_dimension']))\n",
    "            dummy_output = self.conv(dummy_input)\n",
    "            flattened_size = dummy_output.view(1, -1).shape[1]\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(flattened_size, config['dense_neurons']),\n",
    "            activations[config['dense_activation']],\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(config['dense_neurons'], config['num_classes'])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class TrainAndPredict:\n",
    "    def __init__(self, model, device, class_names, optimizer=None, lr=0.001, weight_decay=0.0):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.class_names = class_names\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optimizer_dict[optimizer](self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    def train(self, train_loader, val_loader, epochs=10, save_path='best_model.pth'):\n",
    "        best_val_acc = 0.0\n",
    "        best_epoch = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            correct, total = 0, 0\n",
    "\n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "            train_loss = total_loss / len(train_loader)\n",
    "            train_acc = 100 * correct / total\n",
    "            val_acc = self.validate(val_loader)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "\n",
    "            # Save best model\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_epoch = epoch + 1\n",
    "                torch.save(self.model.state_dict(), save_path)\n",
    "\n",
    "                artifact = wandb.Artifact('best-model', type='model')\n",
    "                artifact.add_file(save_path)\n",
    "                wandb.log_artifact(artifact)\n",
    "\n",
    "            # Log to Weights & Biases\n",
    "            wandb.log({\n",
    "                'epoch': epoch + 1,\n",
    "                'train_loss': train_loss,\n",
    "                'train_acc': train_acc,\n",
    "                'val_acc': val_acc\n",
    "            })\n",
    "\n",
    "        print(f\"\\nBest model saved from Epoch {best_epoch} with Val Acc: {best_val_acc:.2f}%\")\n",
    "\n",
    "    def validate(self, val_loader):\n",
    "        self.model.eval()\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(images)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        return 100 * correct / total\n",
    "\n",
    "    def predict(self, image_tensor):\n",
    "        self.model.eval()\n",
    "        image_tensor = image_tensor.to(self.device).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(image_tensor)\n",
    "            _, pred = torch.max(outputs, 1)\n",
    "        \n",
    "        return self.class_names[pred.item()]\n",
    "\n",
    "\n",
    "\n",
    "def train_sweep(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        # print(config.conv_filters)\n",
    "        wandb.run.name = f\"filter_{config.filter_size}/dn_{config.n_neurons}/opt_{config.optimizer}/aug_{config.use_augmentation}\"\n",
    "\n",
    "        # Build dynamic config from sweep values\n",
    "        dynamic_config = {\n",
    "            'input_dimension': (3, 224, 224),\n",
    "            'conv_filters': config.conv_filters,\n",
    "            'kernel_sizes': [config.filter_size] * 5,\n",
    "            'stride': config.stride,\n",
    "            'filter_org': config.filter_org,\n",
    "            'padding': config.padding,\n",
    "            'max_pooling_size': config.max_pooling_size,\n",
    "            'dropout_rate': config.dropout_rate,\n",
    "            'use_batchnorm': config.use_batchnorm,\n",
    "            'factor': config.factor,\n",
    "            'dropout_organisation': 'after_relu',\n",
    "            'dense_neurons': config.n_neurons,\n",
    "            'num_classes': config.n_classes,\n",
    "            'optimizer': config.optimizer,\n",
    "            'conv_activation': config.conv_activation,\n",
    "            'dense_activation': config.dense_activation,\n",
    "            'image_size': (224, 224),\n",
    "            \n",
    "        }\n",
    "        \n",
    "        if config['filter_org'] == 'half' and config['conv_filters'] < 32:\n",
    "            print(\"Skipping config: unsafe filter_org with too few filters\")\n",
    "            return\n",
    "        if config['stride'] > 1 and config['max_pooling_size'] > 1 and config['filter_size'] >= 7:\n",
    "            print(\"Skipping config: stride/pool too aggressive with large filter\")\n",
    "            return\n",
    "\n",
    "        # Define your model\n",
    "        model = CNN(dynamic_config)\n",
    "\n",
    "        # Dataloaders\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(dynamic_config['image_size'], scale=(0.5, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=15),\n",
    "            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.05),\n",
    "            transforms.RandomGrayscale(p=0.1),\n",
    "            transforms.GaussianBlur(kernel_size=3),\n",
    "            transforms.ToTensor(),\n",
    "        ]) if config.use_augmentation else transforms.Compose([\n",
    "            transforms.Resize(dynamic_config['image_size']),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        val_transform = transforms.Compose([\n",
    "            transforms.Resize(dynamic_config['image_size']),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "        torch.cuda.set_device(device)\n",
    "        train_dataset = datasets.ImageFolder('train', transform=train_transform)\n",
    "        val_dataset = datasets.ImageFolder('val', transform=val_transform)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True,num_workers=4, pin_memory=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=True,num_workers=4, pin_memory=True)\n",
    "\n",
    "        trainer = TrainAndPredict(model, device, train_dataset.classes,optimizer=config.optimizer,lr=config.learning_rate)\n",
    "\n",
    "        # Train and log\n",
    "        trainer.train(train_loader, val_loader, epochs=config.epochs)\n",
    "        \n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'name': 'Custom CNN',\n",
    "    'metric': {'name': \"val_accuracy\", 'goal': 'maximize'},\n",
    "    'parameters': {\n",
    "        'conv_filters': {'values': [32, 64, 128]},\n",
    "        'filter_org': {\n",
    "            'values': ['same', 'double', 'half']\n",
    "        },\n",
    "        'filter_size': {'values': [1,3,7,11]},\n",
    "        'stride': {'values': [1,2]},\n",
    "        'padding': {'values': [1,2]},\n",
    "        'max_pooling_size': {'value': 2},\n",
    "        'n_neurons': {'values': [64, 128, 256, 512, 1024]},\n",
    "        'n_classes': {'value': 10},\n",
    "        'conv_activation': {\n",
    "            'values': ['relu', 'gelu', 'silu', 'mish', 'relu6','leaky_relu']\n",
    "        },\n",
    "        'dense_activation': {\n",
    "            'values': ['relu', 'gelu', 'silu', 'mish', 'relu6','leaky_relu']\n",
    "        },\n",
    "        'dropout_rate': {'values': [0.2, 0.3, 0.4, 0.5]},\n",
    "        'use_batchnorm': {'values': [True, False]},\n",
    "        'factor': {'values': [0.5, 1, 2, 3]},\n",
    "        'learning_rate': {'values': [0.001,0.0001]},\n",
    "        'batch_size': {'values': [16,32,64]},\n",
    "        'optimizer': {'values': ['adam', 'adamw','sgd']},\n",
    "        'epochs': {'values': [5,10,15]},\n",
    "        'use_augmentation': {'values': [True, False]},\n",
    "        'dropout_organisation': {'values': ['after_relu','before_relu']},  # simplified for now\n",
    "    },\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"iNaturalist_CNN\")\n",
    "wandb.agent(sweep_id, function=train_sweep, count=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d198d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "activations = {\n",
    "    'relu': nn.ReLU(),\n",
    "    'tanh': nn.Tanh(),\n",
    "    'sigmoid': nn.Sigmoid(),\n",
    "    'leaky_relu': nn.LeakyReLU(),\n",
    "    'mish':nn.Mish(),\n",
    "    'gelu':nn.GELU(),\n",
    "    'silu':nn.SiLU(),\n",
    "    'relu6':nn.ReLU6()\n",
    "}\n",
    "\n",
    "optimizer_dict = {\n",
    "    'adam': optim.Adam,\n",
    "    'adamw': optim.AdamW,\n",
    "    'sgd': optim.SGD\n",
    "}\n",
    "\n",
    "\n",
    "def generate_filters(base_m, strategy):\n",
    "            if strategy == 'same':\n",
    "                return [base_m] * 5\n",
    "            elif strategy == 'double':\n",
    "                return [base_m * (2 ** i) for i in range(5)]\n",
    "            elif strategy == 'half':\n",
    "                return [max(1, base_m // (2 ** i)) for i in range(5)]\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        in_channels = config['input_dimension'][0]\n",
    "        base_m = config['conv_filters']\n",
    "        strategy = config['filter_org']\n",
    "        conv_filters = generate_filters(base_m, strategy)\n",
    "        kernel_sizes = config['kernel_sizes']\n",
    "        stride = config['stride']\n",
    "        padding = config['padding']\n",
    "        pool = config['max_pooling_size']\n",
    "        dropout = config['dropout_rate']\n",
    "        use_bn = config['use_batchnorm']\n",
    "        dropout_org = config['dropout_organisation']\n",
    "\n",
    "        conv_layers = []\n",
    "        for i in range(5):  # 5 conv layers\n",
    "            out_channels = conv_filters[i]\n",
    "            conv_layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_sizes[i], stride=stride, padding=padding))\n",
    "            if use_bn:\n",
    "                conv_layers.append(nn.BatchNorm2d(out_channels))\n",
    "            if dropout_org == 'before_relu':\n",
    "                conv_layers.append(nn.Dropout2d(dropout))\n",
    "            conv_layers.append(activations[config['conv_activation']])\n",
    "            if dropout_org == 'after_relu':\n",
    "                conv_layers.append(nn.Dropout2d(dropout))\n",
    "            conv_layers.append(nn.MaxPool2d(kernel_size=pool))\n",
    "            in_channels = out_channels\n",
    "\n",
    "        self.conv = nn.Sequential(*conv_layers)\n",
    "\n",
    "        # Estimate flattened size\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros((1, *config['input_dimension']))\n",
    "            dummy_output = self.conv(dummy_input)\n",
    "            flattened_size = dummy_output.view(1, -1).shape[1]\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(flattened_size, config['dense_neurons']),\n",
    "            activations[config['dense_activation']],\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(config['dense_neurons'], config['num_classes'])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class TrainAndPredict:\n",
    "    def __init__(self, model, device, class_names, optimizer=None, lr=0.001, weight_decay=0.0):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.class_names = class_names\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optimizer_dict[optimizer](self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    def train(self, train_loader, val_loader, epochs=10, save_path='best_model.pth'):\n",
    "        best_val_acc = 0.0\n",
    "        best_epoch = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            correct, total = 0, 0\n",
    "\n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "            train_loss = total_loss / len(train_loader)\n",
    "            train_acc = 100 * correct / total\n",
    "            val_acc = self.validate(val_loader)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "\n",
    "            # Save best model\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_epoch = epoch + 1\n",
    "                torch.save(self.model.state_dict(), save_path)\n",
    "\n",
    "                artifact = wandb.Artifact('best-model', type='model')\n",
    "                artifact.add_file(save_path)\n",
    "                wandb.log_artifact(artifact)\n",
    "\n",
    "            # Log to Weights & Biases\n",
    "            wandb.log({\n",
    "                'epoch': epoch + 1,\n",
    "                'train_loss': train_loss,\n",
    "                'train_acc': train_acc,\n",
    "                'val_acc': val_acc\n",
    "            })\n",
    "\n",
    "        print(f\"\\nBest model saved from Epoch {best_epoch} with Val Acc: {best_val_acc:.2f}%\")\n",
    "\n",
    "    def validate(self, val_loader):\n",
    "        self.model.eval()\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(images)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        return 100 * correct / total\n",
    "\n",
    "    def predict(self, image_tensor):\n",
    "        self.model.eval()\n",
    "        image_tensor = image_tensor.to(self.device).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(image_tensor)\n",
    "            _, pred = torch.max(outputs, 1)\n",
    "        \n",
    "        return self.class_names[pred.item()]\n",
    "\n",
    "\n",
    "\n",
    "def train_sweep(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        # print(config.conv_filters)\n",
    "        wandb.run.name = f\"filter_{config.filter_size}/dn_{config.n_neurons}/opt_{config.optimizer}/aug_{config.use_augmentation}\"\n",
    "\n",
    "        # Build dynamic config from sweep values\n",
    "        dynamic_config = {\n",
    "            'input_dimension': (3, 224, 224),\n",
    "            'conv_filters': config.conv_filters,\n",
    "            'kernel_sizes': [config.filter_size] * 5,\n",
    "            'stride': config.stride,\n",
    "            'filter_org': config.filter_org,\n",
    "            'padding': config.padding,\n",
    "            'max_pooling_size': config.max_pooling_size,\n",
    "            'dropout_rate': config.dropout_rate,\n",
    "            'use_batchnorm': config.use_batchnorm,\n",
    "            'factor': config.factor,\n",
    "            'dropout_organisation': 'after_relu',\n",
    "            'dense_neurons': config.n_neurons,\n",
    "            'num_classes': config.n_classes,\n",
    "            'optimizer': config.optimizer,\n",
    "            'conv_activation': config.conv_activation,\n",
    "            'dense_activation': config.dense_activation,\n",
    "            'image_size': (224, 224),\n",
    "            \n",
    "        }\n",
    "        \n",
    "        if config['filter_org'] == 'half' and config['conv_filters'] < 32:\n",
    "            print(\"Skipping config: unsafe filter_org with too few filters\")\n",
    "            return\n",
    "        if config['stride'] > 1 and config['max_pooling_size'] > 1 and config['filter_size'] >= 7:\n",
    "            print(\"Skipping config: stride/pool too aggressive with large filter\")\n",
    "            return\n",
    "\n",
    "        # Define your model\n",
    "        model = CNN(dynamic_config)\n",
    "\n",
    "        # Dataloaders\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(dynamic_config['image_size'], scale=(0.5, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=15),\n",
    "            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.05),\n",
    "            transforms.RandomGrayscale(p=0.1),\n",
    "            transforms.GaussianBlur(kernel_size=3),\n",
    "            transforms.ToTensor(),\n",
    "        ]) if config.use_augmentation else transforms.Compose([\n",
    "            transforms.Resize(dynamic_config['image_size']),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        val_transform = transforms.Compose([\n",
    "            transforms.Resize(dynamic_config['image_size']),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "        torch.cuda.set_device(device)\n",
    "        train_dataset = datasets.ImageFolder('train', transform=train_transform)\n",
    "        val_dataset = datasets.ImageFolder('val', transform=val_transform)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True,num_workers=4, pin_memory=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=True,num_workers=4, pin_memory=True)\n",
    "\n",
    "        trainer = TrainAndPredict(model, device, train_dataset.classes,optimizer=config.optimizer,lr=config.learning_rate)\n",
    "\n",
    "        # Train and log\n",
    "        trainer.train(train_loader, val_loader, epochs=config.epochs)\n",
    "        \n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'name': 'Custom CNN',\n",
    "    'metric': {'name': \"val_accuracy\", 'goal': 'maximize'},\n",
    "    'parameters': {\n",
    "        'conv_filters': {'values': [32, 64, 128]},\n",
    "        'filter_org': {\n",
    "            'values': ['same', 'double', 'half']\n",
    "        },\n",
    "        'filter_size': {'values': [1,3,7,11]},\n",
    "        'stride': {'values': [1,2]},\n",
    "        'padding': {'values': [1,2]},\n",
    "        'max_pooling_size': {'value': 2},\n",
    "        'n_neurons': {'values': [64, 128, 256, 512, 1024]},\n",
    "        'n_classes': {'value': 10},\n",
    "        'conv_activation': {\n",
    "            'values': ['relu', 'gelu', 'silu', 'mish', 'relu6','leaky_relu']\n",
    "        },\n",
    "        'dense_activation': {\n",
    "            'values': ['relu', 'gelu', 'silu', 'mish', 'relu6','leaky_relu']\n",
    "        },\n",
    "        'dropout_rate': {'values': [0.2, 0.3, 0.4, 0.5]},\n",
    "        'use_batchnorm': {'values': [True, False]},\n",
    "        'factor': {'values': [0.5, 1, 2, 3]},\n",
    "        'learning_rate': {'values': [0.001,0.0001]},\n",
    "        'batch_size': {'values': [16,32,64]},\n",
    "        'optimizer': {'values': ['adam', 'adamw','sgd']},\n",
    "        'epochs': {'values': [5,10,15]},\n",
    "        'use_augmentation': {'values': [True, False]},\n",
    "        'dropout_organisation': {'values': ['after_relu','before_relu']},  # simplified for now\n",
    "    },\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"iNaturalist_CNN\")\n",
    "wandb.agent(sweep_id, function=train_sweep, count=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f00f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
